# Default values for ros-ocp-helm

# Chart name overrides
nameOverride: ""
fullnameOverride: ""

global:
  # Storage class for persistent volumes
  storageClass:
  # Pull policy for images
  pullPolicy: IfNotPresent
  # Pull secrets for private registries
  imagePullSecrets: []
  # Init container images (using alternative registries to avoid docker.io rate limits)
  initContainers:
    waitFor:
      repository: registry.access.redhat.com/ubi9/ubi-minimal
      tag: "latest"
    minioMc:
      repository: quay.io/minio/mc
      tag: "RELEASE.2025-07-21T05-28-08Z"

serviceAccount:
  create: true
  name: ros-ocp-backend

# Auth Configuration
auth:
  provider: "oauth2"

# Database configurations
database:
  ros: # Database configuration for ros-ocp services
    image:
      repository: quay.io/insights-onprem/postgresql
      tag: "16"
    storage:
      size: 10Gi
    host: internal  # Use "internal" for the built-in database service, or specify external hostname
    port: 5432
    name: postgres
    user: postgres
    password: postgres
    sslMode: disable
    url: postgresql://postgres:postgres@db-ros:5432/postgres?sslmode=disable

  kruize: # Database configuration for kruize services
    image:
      repository: quay.io/insights-onprem/postgresql
      tag: "16"
    storage:
      size: 10Gi
    host: internal  # Use "internal" for the built-in database service, or specify external hostname
    port: 5432
    name: postgres
    user: postgres
    password: postgres
    adminUser: postgres
    adminPassword: postgres
    sslMode: disable

  sources: # Database configuration for sources-api services
    image:
      repository: quay.io/insights-onprem/postgresql
      tag: "16"
    storage:
      size: 10Gi
    host: internal  # Use "internal" for the built-in database service, or specify external hostname
    port: 5432
    name: sources_api_development
    user: postgres
    password: postgres

# Kafka Configuration
# IMPORTANT: Kafka/Strimzi deployment is NOT managed by this Helm chart!
# The install-helm-chart.sh script installs Strimzi operator and creates Kafka cluster separately
# This section only contains connection settings for the deployed application
kafka:
  # Bootstrap servers for Kafka cluster
  # This is auto-configured by the install script based on the Kafka cluster it creates
  bootstrapServers: "ros-ocp-kafka-kafka-bootstrap.kafka.svc.cluster.local:9092"
  
  # Security protocol (PLAINTEXT, SSL, SASL_PLAINTEXT, SASL_SSL)
  securityProtocol: "PLAINTEXT"


# Redis (Cache - for Kubernetes deployments)
# Note: OpenShift deployments use Valkey instead (configured in openshift-values.yaml)
redis:
  image:
    repository: quay.io/insights-onprem/redis-ephemeral
    tag: "6"
  maxMemory: 512mb
  maxMemoryPolicy: allkeys-lru

# MinIO (Storage - for Kubernetes deployments)
# Note: OpenShift deployments use ODF instead (configured in openshift-values.yaml)
minio:
  image:
    repository: quay.io/minio/minio
    tag: "RELEASE.2025-07-23T15-54-02Z"
  storage:
    size: 20Gi
  rootUser: minioaccesskey
  rootPassword: miniosecretkey
  ports:
    api: 9000
    console: 9990
  buckets:
    - insights-upload-perma
    - koku-bucket
    - ros-data


# Ingress Service
ingress:
  image:
    repository: quay.io/insights-onprem/insights-ros-ingress
    tag: "latest"
    pullPolicy: Always
  port: 8080
  server:
    port: 8080
    readTimeout: 30
    writeTimeout: 30
    idleTimeout: 120
    debug: false
  upload:
    maxUploadSize: 104857600
    maxMemory: 33554432
    tempDir: "/tmp"
    allowedTypes:
      - "application/vnd.redhat.hccm.upload"
    requireAuth: true   # Will be overridden by environment-specific settings
  storage:
    bucket: "ros-data"
    useSSL: false
    urlExpiration: 172800
    pathPrefix: "ros"
  kafka:
    topic: "hccm.ros.events"
    clientId: "insights-ros-ingress"
    batchSize: 16384
    retries: 3
  # Authentication configuration
  # Note: Authentication is only available on OpenShift with Keycloak/RHSSO.
  # On vanilla Kubernetes/KIND, authentication is automatically disabled regardless of this setting.
  auth:
    enabled: true   # Only applies to OpenShift deployments
    allowedOrgs: []
  logging:
    level: "info"
    format: "json"
    output: "stdout"
  metrics:
    enabled: true
    path: "/metrics"
    port: 8080

  # Validation configuration
  validation:
    hccm:
      # Enable hccm validation service (disabled for KIND/development, enabled for OpenShift/production)
      # This will be overridden by platform detection in deployment-ingress.yaml
      enabled: false
      # HCCM validation service URL (empty for development mode)
      serviceUrl: ""
      # Validation timeout in seconds
      timeout: "30"
      # Number of retries for validation calls
      retries: "3"

# Sources API
sourcesApi:
  image:
    repository: quay.io/insights-onprem/sources-api-go
    tag: "latest"
    pullPolicy: Always
  port: 8000
  logLevel: DEBUG
  bypassRbac: true
  sourcesEnv: prod
  encryptionKey: YWFhYWFhYWFhYWFhYWFhYQ
  platformSourcesEventStreamTopic: platform.sources.event-stream


# Kruize Autotune
kruize:
  image:
    repository: quay.io/redhat-services-prod/kruize-autotune-tenant/autotune
    tag: "d0b4337"
  port: 8080
  env:
    loggingLevel: debug
    rootLoggingLevel: error
    dbConfigFile: /tmp/cdappconfig.json
    dbDriver: "jdbc:postgresql://"
    clusterType: kubernetes
    k8sType: openshift
    authType: ""
    monitoringAgent: "prometheus"
    monitoringService: "prometheus"
    monitoringEndpoint: "prometheus"
    saveToDb: true
    local: true
    logAllHttpReqAndResponse: true
    hibernateDialect: org.hibernate.dialect.PostgreSQLDialect
    hibernateDriver: org.postgresql.Driver
    hibernateC3p0MinSize: 2
    hibernateC3p0MaxSize: 5
    hibernateC3p0Timeout: 300
    hibernateC3p0MaxStatements: 100
    hibernateHbm2ddlAuto: none
    hibernateShowSql: false
    hibernateTimezone: UTC
    plots: true
  # Partition management configuration
  partitions:
    # Enable partition creation as initContainer
    createEnabled: true
    # Enable partition deletion CronJob
    deleteEnabled: true
    # Schedule for partition deletion CronJob (daily at midnight)
    deleteSchedule: "0 0 * * *"
    # Job history limits
    successfulJobsHistoryLimit: 3
    failedJobsHistoryLimit: 1
    loggingLevel: info
    rootLoggingLevel: error
    deletePartitionsThreshold: "16"
    # Resource limits for partition jobs
    resources:
      requests:
        memory: "256Mi"
        cpu: "100m"
      limits:
        memory: "512Mi"
        cpu: "300m"

# ROS-OCP Backend Services
rosocp:
  processor:
    image:
      repository: quay.io/insights-onprem/ros-ocp-backend
      tag: "latest"
      pullPolicy: Always
    metricsPort: 9000
    kafkaConsumerGroupId: rosocp-processor
    kafkaAutoCommit: true
    uploadTopic: hccm.ros.events
    kruizeWaitTime: 120
    serviceName: rosocp-processor
    logLevel: INFO

  recommendationPoller:
    image:
      repository: quay.io/insights-onprem/ros-ocp-backend
      tag: "latest"
      pullPolicy: Always
    metricsPort: 9000
    kafkaConsumerGroupId: rosocp-recommendation-poller
    kafkaAutoCommit: false
    recommendationTopic: rosocp.kruize.recommendations
    logLevel: INFO
    serviceName: rosocp-recommendation-poller
    kruizeWaitTime: 120

  api:
    image:

      repository: quay.io/insights-onprem/ros-ocp-backend
      tag: "latest"
      pullPolicy: Always
    port: 8000
    metricsPort: 9000
    pathPrefix: /api
    rbacEnable: false
    dbPoolSize: 10
    dbMaxOverflow: 20
    serviceName: rosocp-api
    logLevel: INFO

  housekeeper:
    image:

      repository: quay.io/insights-onprem/ros-ocp-backend
      tag: "latest"
      pullPolicy: Always
    serviceName: rosocp-housekeeper-sources
    logLevel: INFO

  partitionCleaner:
    schedule: "0 0 */15 * *"  # Runs at 12:00 AM, every 15 days
    image:

      repository: quay.io/insights-onprem/ros-ocp-backend
      tag: "latest"
      pullPolicy: Always
    serviceName: rosocp-housekeeper-partition
    logLevel: INFO
    resources:
      requests:
        memory: "256Mi"
        cpu: "100m"
      limits:
        memory: "512Mi"
        cpu: "300m"

# Service configuration
service:
  type: ClusterIP

# Ingress configuration for external access (used in vanilla Kubernetes)
serviceIngress:
  className: nginx
  hosts:
    - host: localhost
      paths:
        - path: /
          pathType: Prefix

# Route configuration for external access (automatically used on OpenShift)
serviceRoute:
  annotations:
    # OpenShift-specific route annotations
    haproxy.router.openshift.io/timeout: "30s"
    haproxy.router.openshift.io/rewrite-target: ""
  hosts:
    - host: ""  # Empty host uses cluster's default route domain
      paths:
        - path: /
          pathType: Prefix
  tls:
    # Uncomment to enable TLS termination
    # termination: edge
    # insecureEdgeTerminationPolicy: Redirect

# Resource limits and requests
resources:
  # PostgreSQL minimum: ~128Mi memory, can run with 256Mi for development
  database:
    requests:
      memory: "256Mi"
      cpu: "100m"
    limits:
      memory: "512Mi"
      cpu: "500m"

  # Kafka minimum: ~512Mi memory
  strimzi:
    requests:
      memory: "512Mi"
      cpu: "250m"
    limits:
      memory: "1Gi"
      cpu: "500m"

  # ZooKeeper minimum: ~256Mi memory
  strimziZookeeper:
    requests:
      memory: "256Mi"
      cpu: "100m"
    limits:
      memory: "512Mi"
      cpu: "300m"

  # Regular application services
  application:
    requests:
      memory: "256Mi"
      cpu: "100m"
    limits:
      memory: "512Mi"
      cpu: "300m"

  # Kruize is memory-intensive (Java/ML workload)
  kruize:
    requests:
      memory: "1Gi"
      cpu: "500m"
    limits:
      memory: "2Gi"
      cpu: "1000m"

# Startup and readiness probe configuration
probes:
  initialDelaySeconds: 30
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3

# Monitoring configuration
monitoring:
  enabled: true  # Enable monitoring components (ServiceMonitor, etc.)
  scrapeInterval: 30s  # Prometheus scrape interval for metrics collection

# JWT Authentication configuration
# JWT authentication with Keycloak/RHSSO using Envoy's native JWT filter
# - OpenShift: Automatically enabled (requires Keycloak/RH SSO to be deployed)
# - KIND/Vanilla K8s: Automatically disabled (Keycloak not available)
# Platform detection is automatic via Helm's capability detection.
#
# Architecture: Envoy sidecar validates JWT tokens inline using native jwt_authn filter
# with Lua scripting to extract claims into X-ROS headers for the backend service.
jwt_auth:

  # Envoy proxy configuration (sidecar for JWT validation)
  envoy:
    image:
      repository: registry.redhat.io/openshift-service-mesh/proxyv2-rhel9
      tag: "2.6"
      pullPolicy: IfNotPresent
    port: 9080
    adminPort: 9901
    logLevel: info

    # Resource configuration
    resources:
      limits:
        cpu: 500m
        memory: 256Mi
      requests:
        cpu: 100m
        memory: 128Mi

  # Keycloak configuration for JWT validation
  # NOTE: JWT authentication is ONLY supported on OpenShift with Keycloak/RHSSO.
  # On vanilla Kubernetes/KIND, JWT authentication is automatically disabled.
  keycloak:
    # Keycloak URL - leave empty for automatic runtime discovery (OpenShift only)
    #
    # Runtime Discovery on OpenShift (when url is empty):
    #   1. Searches for Keycloak Route in common namespaces (rhsso, keycloak, sso)
    #   2. Falls back to Keycloak Service discovery (cluster-local)
    #   3. Fails with helpful message if not found
    #
    # Override only if using custom/external Keycloak instance on OpenShift
    url: ""  # e.g., "https://keycloak-rhsso.apps.example.com"

    # Keycloak namespace - leave empty for automatic discovery
    # Set this if Keycloak is deployed in a custom namespace that doesn't match
    # the standard patterns (rhsso, keycloak, sso)
    namespace: ""  # e.g., "my-auth-namespace"

    # Keycloak service port for internal cluster-local discovery fallback
    # Only used when discovering Keycloak via Service (not Route)
    servicePort: 8080

    # Keycloak realm name
    realm: kubernetes

    # Client configuration (for reference - used by Cost Management Operator)
    client:
      id: cost-management-operator

    # JWT audience claims that will be validated
    # Must match the 'aud' claim in incoming JWT tokens
    # Only tokens issued for the cost-management-operator client will be accepted
    audiences:
      - cost-management-operator

    # TLS configuration for Keycloak communication
    tls:
      insecureSkipVerify: false
      # caCert: ""  # Path to CA certificate