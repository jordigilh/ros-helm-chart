name: Helm Chart Quality Test

# ============================================================================
# ⚠️  WORKFLOW DISABLED - Insufficient GitHub Runner Resources
# ============================================================================
#
# This workflow is disabled because the Cost Management stack requires more
# resources than GitHub-hosted runners provide:
#
#   Required:  ~14 Gi memory (OCP-Only minimal), ~24 Gi (full stack)
#   Available: 7 Gi memory (ubuntu-latest)
#
# To run E2E tests, you need one of:
#   1. GitHub Large Runners (16-core with 64 GB RAM) - requires GitHub Enterprise
#   2. Self-hosted runner with 32+ GB RAM
#   3. Manual testing on OpenShift/Kubernetes cluster
#
# See docs/resource-requirements.md for full resource breakdown.
# ============================================================================
#
# Original workflow description:
# This workflow tests the Cost Management On-Premise Helm chart deployment on Kind using the local chart
# Deployment flow:
#   1. Setup KIND cluster
#   2. Deploy Kafka infrastructure (Strimzi + Kafka) - prerequisite
#   3. Deploy Cost Management On-Premise application
#
# Uses alternative registries to avoid Docker Hub rate limiting:
# - public.ecr.aws for official images (postgres, redis, busybox, confluent)
# - quay.io for application images (already configured)
# - ghcr.io as fallback for some images
# 
# Chart Source: Uses local cost-onprem/ directory instead of GitHub releases

on:
  # DISABLED: Uncomment to re-enable when larger runners are available
  # pull_request:
  #   paths:
  #     - 'cost-onprem/**'
  #     - 'scripts/install-helm-chart.sh'
  #     - 'scripts/deploy-strimzi.sh'
  #     - 'scripts/deploy-kind.sh'

  workflow_dispatch:
    # Manual trigger only - for use with self-hosted runners or larger runners

jobs:
  helm-chart-test:
    name: Test Helm Chart Deployment
    runs-on: ubuntu-latest
    timeout-minutes: 30
    env:
      CONTAINER_RUNTIME: podman

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install Podman
        run: |
          # Install Podman
          sudo apt-get update
          sudo apt-get install -y podman

          # Verify Podman installation
          echo "Podman version:"
          podman --version
          echo "Podman info:"
          podman info
          echo "Testing Podman connectivity:"
          podman run --rm quay.io/podman/hello

      - name: Install KIND
        run: |
          # Get the latest KIND version
          KIND_VERSION=$(curl -s https://api.github.com/repos/kubernetes-sigs/kind/releases/latest | grep '"tag_name":' | sed -E 's/.*"([^"]+)".*/\1/')
          echo "Latest KIND version: $KIND_VERSION"

          # Download and install the latest KIND version
          curl -Lo ./kind "https://kind.sigs.k8s.io/dl/${KIND_VERSION}/kind-linux-amd64"
          chmod +x ./kind
          sudo mv ./kind /usr/local/bin/kind

          # Verify installation
          kind version

          # Configure Kind to use Podman explicitly
          echo "Podman driver will be used by Kind"

      - name: Install kubectl
        run: |
          curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
          chmod +x kubectl
          sudo mv kubectl /usr/local/bin/kubectl
          kubectl version --client

      - name: Install Helm
        run: |
          curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
          helm version

      - name: Set up environment and Podman configuration
        run: |
          echo "KIND_CLUSTER_NAME=cost-onprem-test-cluster" >> $GITHUB_ENV
          echo "HELM_RELEASE_NAME=cost-onprem-test" >> $GITHUB_ENV
          echo "NAMESPACE=cost-onprem-test" >> $GITHUB_ENV

      - name: Setup KIND cluster
        timeout-minutes: 15
        run: |
          cd scripts
          export KIND_CLUSTER_NAME=${{ env.KIND_CLUSTER_NAME }}
          export HELM_RELEASE_NAME=${{ env.HELM_RELEASE_NAME }}
          export NAMESPACE=${{ env.NAMESPACE }}

          # Make scripts executable
          chmod +x deploy-kind.sh

          # Setup KIND cluster
          echo "Setting up KIND cluster..."
          ./deploy-kind.sh

          # Verify cluster is running
          echo "Verifying cluster status..."
          kind get clusters
          kubectl cluster-info
          kubectl get nodes -o wide


      - name: Deploy Kafka Infrastructure
        timeout-minutes: 10
        run: |
          cd scripts

          # Make script executable
          chmod +x deploy-strimzi.sh

          # Deploy Strimzi and Kafka (uses default KAFKA_NAMESPACE=kafka)
          echo "Deploying Kafka infrastructure..."
          ./deploy-strimzi.sh

          # Verify Kafka deployment
          echo "Verifying Kafka deployment..."
          kubectl get pods -n kafka
          kubectl get kafka -n kafka

      - name: Deploy Helm Chart
        timeout-minutes: 20
        run: |
          # Use local Helm chart instead of pulling from GitHub
          cd scripts
          export KIND_CLUSTER_NAME=${{ env.KIND_CLUSTER_NAME }}
          export HELM_RELEASE_NAME=${{ env.HELM_RELEASE_NAME }}
          export NAMESPACE=${{ env.NAMESPACE }}
          export USE_LOCAL_CHART=true
          export LOCAL_CHART_PATH=../cost-onprem
          export HELM_TIMEOUT=900s

          # Make scripts executable
          chmod +x install-helm-chart.sh

          # Deploy Helm chart using local chart
          echo "Installing Cost Management On-Premise Helm chart from local source..."
          echo "Using local chart: $LOCAL_CHART_PATH"
          echo "Using Helm timeout: $HELM_TIMEOUT"
          ./install-helm-chart.sh

          # Verify deployment
          echo "Verifying Helm chart deployment..."
          kubectl get pods -n $NAMESPACE
          kubectl get services -n $NAMESPACE
          kubectl get ingress -n $NAMESPACE

      - name: Wait for ingress routes to propagate
        run: |
          echo "Waiting for nginx ingress controller to propagate route configuration..."
          echo "This wait is necessary because:"
          echo "  - Ingress controller watches for new Ingress resources and pod readiness"
          echo "  - It needs time to update internal routing tables"
          echo "  - Complex routes (like '/' prefix) take longer than simple exact matches"
          echo "  - Typical propagation time: 30-60 seconds"
          sleep 45
          echo "✅ Ingress routes should now be propagated"

      - name: Run dataflow test
        run: |
          echo "Running Kubernetes dataflow test..."
          cd scripts
          chmod +x test-k8s-dataflow.sh
          ./test-k8s-dataflow.sh

      - name: Show cluster status on failure
        if: failure()
        run: |
          echo "=== Container Runtime Status ==="
          ${{ env.CONTAINER_RUNTIME }} --version || true
          ${{ env.CONTAINER_RUNTIME }} info || true
          ${{ env.CONTAINER_RUNTIME }} ps -a || true
          ${{ env.CONTAINER_RUNTIME }} images | head -10 || true

          echo "=== Kind Status ==="
          kind version || true
          kind get clusters || true

          echo "=== Cluster Status ==="
          kubectl cluster-info || true
          kubectl version || true

          echo "=== Kafka Infrastructure Status ==="
          echo "Kafka Pods:"
          kubectl get pods -n kafka -o wide || true
          echo "Kafka Cluster:"
          kubectl get kafka -n kafka || true
          echo "Kafka Topics:"
          kubectl get kafkatopic -n kafka || true
          echo "Strimzi Operator:"
          kubectl get pods -n kafka -l name=strimzi-cluster-operator || true
          echo "Kafka Events:"
          kubectl get events -n kafka --sort-by='.lastTimestamp' || true

          echo "=== Cost Management On-Premise Pods Status ==="
          kubectl get pods -n ${{ env.NAMESPACE }} -o wide || true
          kubectl get pods --all-namespaces | head -20 || true

          echo "=== Services Status ==="
          kubectl get services -n ${{ env.NAMESPACE }} || true

          echo "=== Events ==="
          kubectl get events -n ${{ env.NAMESPACE }} --sort-by='.lastTimestamp' || true
          kubectl get events --all-namespaces --sort-by='.lastTimestamp' | head -20 || true

          echo "=== Persistent Volumes ==="
          kubectl get pv,pvc -n ${{ env.NAMESPACE }} || true
          kubectl get pv,pvc -n kafka || true

          echo "=== Node Status ==="
          kubectl get nodes -o wide || true
          kubectl describe nodes || true

          echo "=== Authentication Status ==="
          echo "Service Accounts:"
          kubectl get serviceaccounts -n ${{ env.NAMESPACE }} || true
          echo "Service Account Details:"
          kubectl describe serviceaccount insights-ros-ingress -n ${{ env.NAMESPACE }} || true
          echo "Secrets:"
          kubectl get secrets -n ${{ env.NAMESPACE }} | grep -E "(insights-ros-ingress|token)" || true
          echo "All Secrets:"
          kubectl get secrets -n ${{ env.NAMESPACE }} || true
          echo "Authentication files:"
          ls -la /tmp/ | grep -E "(kubeconfig|auth)" || true
          echo "Testing TokenRequest API:"
          kubectl create token insights-ros-ingress -n ${{ env.NAMESPACE }} --duration=60s --dry-run=server || true
          echo "RBAC Check:"
          kubectl auth can-i get pods --as=system:serviceaccount:${{ env.NAMESPACE }}:insights-ros-ingress -n ${{ env.NAMESPACE }} || true

          echo "=== Kafka/Strimzi Logs ==="
          echo "Strimzi Operator Logs:"
          kubectl logs -n kafka -l name=strimzi-cluster-operator --tail=50 || true
          echo "Kafka Broker Logs:"
          kubectl logs -n kafka -l strimzi.io/name=onprem-kafka-kafka --tail=30 || true
          
          echo "=== Ingress Service Logs ==="
          kubectl logs -n ${{ env.NAMESPACE }} -l app.kubernetes.io/component=ingress --tail=30 || true
          echo "=== Recent Logs ==="
          for pod in $(kubectl get pods -n ${{ env.NAMESPACE }} -o name | head -5); do
            echo "--- Logs for $pod ---"
            kubectl logs -n ${{ env.NAMESPACE }} $pod --tail=20 || true
            kubectl logs -n ${{ env.NAMESPACE }} $pod --previous --tail=10 || true
          done

          echo "=== Container Logs ==="
          for container in $(${{ env.CONTAINER_RUNTIME }} ps --filter "label=io.x-k8s.kind.cluster=${{ env.KIND_CLUSTER_NAME }}" --format "{{.Names}}" | head -3); do
            echo "--- ${{ env.CONTAINER_RUNTIME }} logs for $container ---"
            ${{ env.CONTAINER_RUNTIME }} logs $container --tail=20 || true
          done

          echo "=== System Resources ==="
          df -h || true
          free -h || true
          top -bn1 | head -20 || true

      - name: Cleanup
        if: always()
        run: |
          kind delete cluster --name ${{ env.KIND_CLUSTER_NAME }} || true
