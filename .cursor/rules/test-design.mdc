---
description: Test design principles and guidelines for writing pytest tests
globs: ["tests/**/*.py"]
---

# Test Design Guidelines

## Core Principle: Tests Must Be Self-Contained

**Every test must set up its own data and clean up after itself.**

Tests MUST NOT depend on:
- Data created by other tests
- Leftover data from previous test runs
- Specific test execution order

### Why This Matters
- Tests run in parallel or random order
- CI environments start fresh each run
- Flaky tests waste developer time
- Dependencies create cascading failures

## Correct Patterns

### ✅ Self-Contained Test with Fixture
```python
@pytest.fixture(scope="module")
def test_data(cluster_config):
    """Set up test data, yield context, clean up."""
    # Setup
    cluster_id = generate_cluster_id()
    source = register_source(cluster_id)
    upload_data(cluster_id)
    wait_for_processing(cluster_id)
    
    yield {"cluster_id": cluster_id, "source": source}
    
    # Cleanup
    delete_source(source.id)
    cleanup_database(cluster_id)

def test_something(test_data):
    """Test uses data from fixture."""
    assert query_data(test_data["cluster_id"]) is not None
```

### ✅ Test That Skips When Prerequisites Missing
```python
def test_requires_data(cluster_config):
    """Skip if required infrastructure not available."""
    pod = get_pod_by_label(cluster_config.namespace, "app=database")
    if not pod:
        pytest.skip("Database pod not found")
    
    # Actual test logic
    assert pod_is_healthy(pod)
```

## Anti-Patterns to Avoid

### ❌ Test Depending on Another Test's Data
```python
# BAD: Assumes test_01 already ran and created data
def test_02_verify_data():
    result = query_database("SELECT * FROM data")
    assert len(result) > 0  # Will fail if test_01 didn't run
```

### ❌ Test That Passes When It Should Skip
```python
# BAD: Returns early without skip or assertion
def test_something():
    data = get_existing_data()
    if not data:
        return  # Silent pass - should be pytest.skip()
    assert data["value"] > 0
```

### ❌ Hardcoded Dependencies on Test Order
```python
# BAD: Assumes specific cluster_id from previous test
def test_verify_cluster():
    cluster_id = "e2e-pytest-12345"  # Hardcoded from test_01
    assert cluster_exists(cluster_id)
```

## Shared E2E Helpers

Use `tests/e2e_helpers.py` for common setup/teardown:

| Component | Usage |
|-----------|-------|
| `NISEConfig` | Consistent test data configuration |
| `generate_cluster_id()` | Unique cluster IDs per test |
| `register_source()` | Source registration with cleanup |
| `wait_for_provider()` | Wait for Kafka → Koku provider creation |
| `cleanup_database_records()` | Clean up test data |

## When Writing New Tests

1. **Ask**: "Can this test run in isolation?"
2. **Use fixtures** for setup/teardown
3. **Generate unique IDs** for test data
4. **Clean up** in fixture teardown or `finally` blocks
5. **Skip gracefully** when prerequisites unavailable
6. **Never assume** data from other tests exists

## Test Markers for Dependencies

If a test truly requires extended setup time:
```python
@pytest.mark.extended
@pytest.mark.timeout(900)
def test_long_running():
    """Marked extended - not run in default CI."""
    pass
```

## Parametrization Guidelines

Use `@pytest.mark.parametrize` to reduce duplicate code while maintaining clear test names.

### ✅ Good: Parametrize with Clear IDs
```python
@pytest.mark.parametrize("resource_type,db_column,expected_key", [
    pytest.param("node", "node", "expected_node_count", id="node_count"),
    pytest.param("namespace", "namespace", "expected_namespace_count", id="namespace_count"),
    pytest.param("pod", "resource_id", "expected_pod_count", id="pod_count"),
])
def test_resource_count_matches_expected(self, data, resource_type, db_column, expected_key):
    """Verify unique resource count matches expected."""
    # Single implementation, clear test names like:
    # test_resource_count_matches_expected[node_count]
    # test_resource_count_matches_expected[namespace_count]
```

### ✅ Good: Parametrize Lists from Class Constants
```python
class TestKafkaTopics:
    REQUIRED_TOPICS = ["platform.upload.announce", "hccm.ros.events"]
    
    @pytest.mark.parametrize("topic", REQUIRED_TOPICS)
    def test_required_topic_exists(self, topic: str):
        """Verify required Kafka topic exists."""
        pass
```

### ❌ Bad: Parametrize Without IDs (Unclear Names)
```python
# BAD: Test names will be test_metric[pod_request_cpu_core_hours]
@pytest.mark.parametrize("metric", [
    "pod_request_cpu_core_hours",
    "pod_request_memory_gigabyte_hours",
])
def test_metric(self, metric):
    pass
```

### When to Parametrize

**DO parametrize when:**
- Multiple tests have identical logic with different inputs
- Testing the same behavior across multiple resources/values
- The test name with `[id]` suffix remains clear

**DON'T parametrize when:**
- Tests have significantly different logic paths
- The parametrized ID would be confusing
- It would obscure what's being tested

### Parametrization Pattern Reference

| Pattern | Example ID | Use Case |
|---------|------------|----------|
| Resource types | `[node_count]`, `[pod_count]` | Counting different resources |
| Metrics | `[cpu_usage]`, `[memory_usage]` | Validating different metrics |
| Required items | `[platform.upload.announce]` | Checking required topics/buckets |
| Scenarios | `[minimal_ocp_pod_only]` | Testing different configurations |
