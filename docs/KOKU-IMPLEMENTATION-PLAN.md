# Koku Integration Implementation Plan
**Aligned with PR #27: cost-management-onprem Chart**

**Status**: ğŸ“‹ **READY FOR REVIEW**
**Date**: November 6, 2025
**Estimated Duration**: 4 weeks
**Complexity**: High (16 Koku templates + 7 Trino templates)

---

## Table of Contents

1. [Executive Summary](#executive-summary)
2. [Architecture Overview](#architecture-overview)
3. [Source Code Analysis](#source-code-analysis)
4. [Implementation Phases](#implementation-phases)
5. [NetworkPolicy Requirements](#networkpolicy-requirements)
6. [values-koku.yaml Structure](#values-kokuyaml-structure)
7. [Template Files](#template-files)
8. [Service Communication Matrix](#service-communication-matrix)
9. [Resource Requirements](#resource-requirements)
10. [Testing Strategy](#testing-strategy)
11. [Risks & Mitigations](#risks--mitigations)

---

## Executive Summary

### Goal
Integrate Koku cost management into the `cost-management-onprem` Helm chart (after PR #27 merges), adding 16 Koku templates and 7 Trino templates, with comprehensive NetworkPolicies for OpenShift.

### Key Stats
- **New Templates**: 23 total (16 Koku + 7 Trino)
- **New Pods**: ~22 (5 API + 13 Workers + 1 Beat + 3 Trino)
- **Resource Requirements**: ~8-14 cores, ~19-34GB RAM (minimal profile)
- **Network Policies**: 15 policies for OCP
- **Confidence**: ğŸŸ¢ **95% HIGH CONFIDENCE**

### Approach
1. **Create `values-koku.yaml`** (separate from PR #27's `values.yaml`)
2. **Add templates** to `cost-management-onprem/templates/cost-management/` and `trino/`
3. **Use PR #27 helpers** (`cost-mgmt.*`)
4. **Add our helpers** (`_helpers-koku.tpl`)
5. **Test** with both values files: `-f values.yaml -f values-koku.yaml`

---

## Architecture Overview

### High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Cost Management On-Premise Platform                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ ROS Components â”‚  â”‚ Koku (NEW)     â”‚  â”‚ Trino (NEW)      â”‚  â”‚
â”‚  â”‚ (PR #27)       â”‚  â”‚                â”‚  â”‚                  â”‚  â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚  â”‚ â€¢ API          â”‚  â”‚ â€¢ API (R/W)    â”‚  â”‚ â€¢ Coordinator    â”‚  â”‚
â”‚  â”‚ â€¢ Processor    â”‚  â”‚ â€¢ Celery Beat  â”‚  â”‚ â€¢ Workers        â”‚  â”‚
â”‚  â”‚ â€¢ Poller       â”‚  â”‚ â€¢ 13 Workers   â”‚  â”‚ â€¢ Metastore      â”‚  â”‚
â”‚  â”‚ â€¢ Housekeeper  â”‚  â”‚                â”‚  â”‚ â€¢ Metastore DB   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Shared Infrastructure (PR #27)                              â”‚â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚
â”‚  â”‚ â€¢ PostgreSQL â€¢ Redis/Valkey â€¢ Kafka â€¢ MinIO â€¢ Ingress      â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Koku Service Architecture

```
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚   Ingress   â”‚
                      â”‚ (insights-  â”‚
                      â”‚ ingress-go) â”‚
                      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚                         â”‚
           â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
           â”‚ Koku API â”‚            â”‚ Koku API â”‚
           â”‚  Reads   â”‚            â”‚  Writes  â”‚
           â”‚ (2 pods) â”‚            â”‚ (1 pod)  â”‚
           â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                â”‚                        â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚              â”‚              â”‚
         â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”
         â”‚PostgreSQLâ”‚    â”‚  Redis  â”‚    â”‚  Kafka â”‚
         â”‚  (koku  â”‚    â”‚ (cache  â”‚    â”‚ (msgs) â”‚
         â”‚   DB)   â”‚    â”‚ &broker)â”‚    â”‚        â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                             â”‚
         â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Celery  â”‚               â”‚ Celery Workers â”‚
         â”‚   Beat   â”‚               â”‚   (13 types)   â”‚
         â”‚ (1 pod)  â”‚               â”‚  â€¢ default     â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚  â€¢ priority    â”‚
                                    â”‚  â€¢ refresh     â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â€¢ summary     â”‚
              â”‚                     â”‚  â€¢ hcs         â”‚
              â”‚                     â”‚  â€¢ *-xl        â”‚
         â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”                â”‚  â€¢ *-penalty   â”‚
         â”‚  Trino  â”‚                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚Coordinatorâ”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  +Workers â”‚                   â”‚
         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜              â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
              â”‚                    â”‚  MinIO   â”‚
         â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”              â”‚(S3-like) â”‚
         â”‚   Hive   â”‚              â”‚ storage  â”‚
         â”‚Metastore â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Source Code Analysis

### Key Findings from Koku Repository

#### 1. **API Server** (`/Users/jgil/go/src/github.com/insights-onprem/koku`)

**Port**: 8000 (HTTP)
**Source**: `run_server.sh`, `gunicorn_conf.py`

```bash
# run_server.sh
gunicorn koku.wsgi --bind=0.0.0.0:8000
```

**Health Checks**:
- Liveness: `GET /api/cost-management/v1/status/`
- Readiness: `GET /api/cost-management/v1/status/`
- Source: `deploy/clowdapp.yaml:124,134`

#### 2. **Database Configuration** (`koku/koku/configurator.py`)

**Connection via**: `CONFIGURATOR.get_database_host()` / `.get_database_port()`
**Default Port**: 5432 (PostgreSQL)
**Database Name**: `koku` (separate from ROS database)

```python
# koku/koku/settings.py:324
DATABASES = {"default": database.config()}
```

#### 3. **Redis Configuration** (`koku/koku/settings.py:224-227`)

**Purpose**: Cache + Celery Broker + Result Backend
**Connection**:

```python
REDIS_HOST = CONFIGURATOR.get_in_memory_db_host()  # Default: "redis"
REDIS_PORT = CONFIGURATOR.get_in_memory_db_port()  # Default: 6379
REDIS_DB = 1
REDIS_URL = f"redis://{REDIS_HOST}:{REDIS_PORT}/{REDIS_DB}"
```

**Uses**:
- Django cache (default, api, rbac, worker)
- Celery broker: `CELERY_BROKER_URL = REDIS_URL`
- Celery result backend: `CELERY_RESULTS_URL`

#### 4. **Kafka Configuration** (`koku/koku/configurator.py:206-211`)

**Purpose**: Message queue for source events
**Connection**:

```python
def get_kafka_broker_list():
    return [
        f'{ENVIRONMENT.get_value("INSIGHTS_KAFKA_HOST", default="localhost")}:'
        f'{ENVIRONMENT.get_value("INSIGHTS_KAFKA_PORT", default="29092")}'
    ]
```

**Default Port**: 29092 (Kafka plaintext)

#### 5. **S3/MinIO Configuration** (`koku/koku/settings.py:590-618`)

**Purpose**: Object storage for cost reports (Parquet files)
**Connection**:

```python
S3_ENDPOINT = CONFIGURATOR.get_object_store_endpoint()
S3_BUCKET_NAME = CONFIGURATOR.get_object_store_bucket(REQUESTED_BUCKET)
S3_ACCESS_KEY = CONFIGURATOR.get_object_store_access_key(REQUESTED_BUCKET)
S3_SECRET = CONFIGURATOR.get_object_store_secret_key(REQUESTED_BUCKET)
```

**Buckets**:
- `koku-report` (main bucket)
- `ros-report` (ROS integration)
- `subs-report` (subscriptions)

#### 6. **Trino Configuration** (`koku/koku/settings.py:624-627`)

**Purpose**: Query Parquet files in S3/MinIO
**Connection**:

```python
TRINO_HOST = ENVIRONMENT.get_value("TRINO_HOST", default=None)
TRINO_PORT = ENVIRONMENT.get_value("TRINO_PORT", default=None)  # Default: 8080
```

**Usage**: `koku/trino_database.py:197-198`

```python
"host": (connect_args.get("host") or os.environ.get("TRINO_HOST") or "trino"),
"port": (connect_args.get("port") or os.environ.get("TRINO_PORT") or 8080),
```

#### 7. **Celery Configuration** (`koku/koku/celery.py`)

**Broker**: Redis (same as cache)
**Workers**: 13 types (from ClowdApp manifest)
- Essential: default, priority, refresh, summary, hcs
- XL: priority-xl, refresh-xl, summary-xl
- Penalty: priority-penalty, refresh-penalty, summary-penalty
- Disabled: subs-extraction, subs-transmission (not deployed)

**Beat Scheduler**: 1 replica (must be exactly 1)

---

## Implementation Phases

### Phase 1: Setup and Values File (Days 1-2)

**Duration**: 2 days
**Confidence**: ğŸŸ¢ **99% (Very High)**

#### Tasks

1. **Create `values-koku.yaml`**
   - Koku API configuration (reads + writes)
   - Celery configuration (beat + 13 workers)
   - Koku database configuration
   - Trino configuration (minimal profile)
   - All with minimal resource requirements

2. **Create `_helpers-koku.tpl`**
   - Koku-specific helper functions
   - Extend PR #27's `cost-mgmt.*` helpers
   - Service name generation
   - Database name resolution

3. **Update PR #27's `values.yaml`** (after merge)
   - Ensure `costManagement` placeholder has proper structure
   - Ensure `trino` section exists (if not, add via `values-koku.yaml`)

#### Deliverables

- `cost-management-onprem/values-koku.yaml` âœ…
- `cost-management-onprem/templates/_helpers-koku.tpl` âœ…
- Documentation: `docs/KOKU-VALUES-REFERENCE.md` âœ…

---

### Phase 2: Koku Core Services (Days 3-7)

**Duration**: 5 days
**Confidence**: ğŸŸ¢ **95% (High)**

#### Tasks

1. **Koku Database** (Day 3)
   - StatefulSet for PostgreSQL (dedicated for Koku)
   - Service
   - PVC (20Gi)
   - Init scripts (schema creation)

2. **Django Secret** (Day 3)
   - Auto-generate 50-character alphanumeric string
   - Store in Secret
   - Stable on upgrades (using `lookup` function)

3. **Koku API - Reads** (Days 4-5)
   - Deployment (2 replicas)
   - Service
   - Environment variables (25+ vars)
   - Health checks (liveness + readiness)
   - Volume mounts (tmp, aws creds, gcp creds)

4. **Koku API - Writes** (Days 4-5)
   - Deployment (1 replica)
   - Service
   - Same env vars as reads
   - Same health checks

5. **ServiceMonitor** (Day 6)
   - Prometheus scraping for Koku API metrics
   - Extends existing monitoring setup

6. **Initial Testing** (Day 7)
   - Deploy with `helm install -f values.yaml -f values-koku.yaml`
   - Verify API pods start
   - Verify database connectivity
   - Verify Redis connectivity

#### Deliverables

- `templates/cost-management/database/statefulset.yaml` âœ…
- `templates/cost-management/database/service.yaml` âœ…
- `templates/cost-management/secrets/django-secret.yaml` âœ…
- `templates/cost-management/secrets/database-credentials.yaml` âœ…
- `templates/cost-management/api/deployment-reads.yaml` âœ…
- `templates/cost-management/api/deployment-writes.yaml` âœ…
- `templates/cost-management/api/service.yaml` âœ…
- `templates/cost-management/monitoring/servicemonitor.yaml` âœ…

---

### Phase 3: Celery Workers (Days 8-12)

**Duration**: 5 days
**Confidence**: ğŸŸ¢ **92% (High)**

#### Tasks

1. **Celery Beat** (Day 8)
   - Deployment (1 replica - MUST be exactly 1)
   - Environment variables
   - Liveness/readiness probes (check Redis connection)
   - Connect to Redis, PostgreSQL

2. **Essential Workers** (Days 9-10)
   - `default`, `priority`, `refresh`, `summary`, `hcs`
   - 5 Deployment templates
   - Each with specific queue configuration
   - Minimal resources (100m CPU, 200Mi RAM)

3. **XL Workers** (Day 11)
   - `priority-xl`, `refresh-xl`, `summary-xl`
   - 3 Deployment templates
   - Same resource requirements as essential (minimal for dev)

4. **Penalty Workers** (Day 11)
   - `priority-penalty`, `refresh-penalty`, `summary-penalty`
   - 3 Deployment templates
   - Same resource requirements

5. **Worker Testing** (Day 12)
   - Verify all 13 workers start
   - Verify Celery beat connects to Redis
   - Verify workers process tasks from queues
   - Check resource utilization

#### Deliverables

- `templates/cost-management/celery/deployment-beat.yaml` âœ…
- `templates/cost-management/celery/deployment-worker-default.yaml` âœ…
- `templates/cost-management/celery/deployment-worker-priority.yaml` âœ…
- `templates/cost-management/celery/deployment-worker-refresh.yaml` âœ…
- `templates/cost-management/celery/deployment-worker-summary.yaml` âœ…
- `templates/cost-management/celery/deployment-worker-hcs.yaml` âœ…
- `templates/cost-management/celery/deployment-worker-priority-xl.yaml` âœ…
- `templates/cost-management/celery/deployment-worker-refresh-xl.yaml` âœ…
- `templates/cost-management/celery/deployment-worker-summary-xl.yaml` âœ…
- `templates/cost-management/celery/deployment-worker-priority-penalty.yaml` âœ…
- `templates/cost-management/celery/deployment-worker-refresh-penalty.yaml` âœ…
- `templates/cost-management/celery/deployment-worker-summary-penalty.yaml` âœ…
- `templates/cost-management/celery/deployment-worker-summary-penalty.yaml` âœ…

---

### Phase 4: Trino Integration (Days 13-17)

**Duration**: 5 days
**Confidence**: ğŸŸ¢ **90% (High)** - Using existing `trino-chart/` work

#### Tasks

1. **Hive Metastore Database** (Day 13)
   - StatefulSet for PostgreSQL (dedicated for metastore)
   - Service
   - PVC (2Gi)

2. **Hive Metastore** (Day 14)
   - Deployment (1 replica)
   - Service (port 9083)
   - ConfigMap (metastore configuration)
   - Environment variables (DB connection)

3. **Trino Coordinator** (Days 15-16)
   - StatefulSet (1 replica)
   - Service (port 8080)
   - ConfigMap (coordinator config)
   - Catalogs: Hive + PostgreSQL
   - JVM settings (minimal: 1GB heap)
   - PVC (5Gi for spill)

4. **Trino Workers** (Days 15-16)
   - Deployment (1 replica for minimal)
   - ConfigMap (worker config)
   - JVM settings (minimal: 1GB heap)
   - PVC (5Gi for spill)

5. **Trino Testing** (Day 17)
   - Verify coordinator starts
   - Verify worker connects to coordinator
   - Verify metastore connectivity
   - Test Hive catalog (query Parquet from MinIO)
   - Test PostgreSQL catalog (query Koku DB)
   - Test cross-catalog query

#### Deliverables

- `templates/trino/metastore/database/statefulset.yaml` âœ…
- `templates/trino/metastore/database/service.yaml` âœ…
- `templates/trino/metastore/deployment.yaml` âœ…
- `templates/trino/metastore/service.yaml` âœ…
- `templates/trino/metastore/configmap.yaml` âœ…
- `templates/trino/coordinator/statefulset.yaml` âœ…
- `templates/trino/coordinator/service.yaml` âœ…
- `templates/trino/coordinator/configmap.yaml` âœ…
- `templates/trino/worker/deployment.yaml` âœ…
- `templates/trino/worker/configmap.yaml` âœ…
- `templates/trino/monitoring/servicemonitor.yaml` âœ…

---

### Phase 5: NetworkPolicies (Days 18-20)

**Duration**: 3 days
**Confidence**: ğŸŸ¢ **93% (High)**

#### Tasks

1. **Analyze Service Communication** (Day 18)
   - Map all communication paths
   - Document ports and protocols
   - Create NetworkPolicy matrix

2. **Create NetworkPolicy Templates** (Days 18-19)
   - 15 NetworkPolicy resources (OCP only)
   - Ingress rules for each service
   - Egress rules for each service
   - DNS egress for all

3. **Testing** (Day 20)
   - Deploy with NetworkPolicies enabled
   - Verify all services communicate
   - Verify blocked traffic is actually blocked
   - Test with `kubectl exec` and `curl`

#### Deliverables

- `templates/cost-management/api/networkpolicy.yaml` âœ…
- `templates/cost-management/celery/networkpolicy-beat.yaml` âœ…
- `templates/cost-management/celery/networkpolicy-workers.yaml` âœ…
- `templates/cost-management/database/networkpolicy.yaml` âœ…
- `templates/trino/coordinator/networkpolicy.yaml` âœ…
- `templates/trino/worker/networkpolicy.yaml` âœ…
- `templates/trino/metastore/networkpolicy.yaml` âœ…
- `templates/trino/metastore/database/networkpolicy.yaml` âœ…
- Documentation: `docs/KOKU-NETWORKPOLICIES.md` âœ…

---

### Phase 6: Final Integration & Testing (Days 21-28)

**Duration**: 8 days
**Confidence**: ğŸŸ¢ **85% (High)** - Some unknowns in full integration

#### Tasks

1. **Integration Testing** (Days 21-23)
   - Deploy full stack on `stress.parodos.dev`
   - Create test sources
   - Upload sample cost data to MinIO
   - Trigger Celery workers to process data
   - Verify Trino queries work
   - Verify API responses

2. **Documentation** (Days 24-25)
   - `docs/KOKU-INSTALLATION.md`: Complete installation guide
   - `docs/KOKU-CONFIGURATION.md`: values.yaml reference
   - `docs/KOKU-TROUBLESHOOTING.md`: Common issues
   - Update main `README.md`
   - Update `docs/README.md`

3. **Polish** (Days 26-27)
   - Add NOTES.txt for Koku services
   - Add helm test hooks (optional)
   - Cleanup temporary files
   - Ensure linting passes
   - Final resource optimization

4. **Pull Request** (Day 28)
   - Create PR against main (after #27 merges)
   - Comprehensive PR description
   - Link to all documentation
   - Request review from team

#### Deliverables

- Complete Koku integration âœ…
- Full documentation set âœ…
- Tested on OpenShift cluster âœ…
- PR ready for review âœ…

---

## NetworkPolicy Requirements

### Communication Matrix

| Source | Destination | Port | Protocol | Purpose |
|--------|-------------|------|----------|---------|
| **Koku API (R)** | PostgreSQL (koku) | 5432 | TCP | Database queries (read-only) |
| **Koku API (R)** | Redis | 6379 | TCP | Cache reads |
| **Koku API (R)** | Kafka | 29092 | TCP | Consume source events |
| **Koku API (R)** | MinIO | 9000 | TCP | Read cost reports |
| **Koku API (R)** | Trino | 8080 | TCP | Query Parquet data |
| **Koku API (R)** | ROS API | 8000 | TCP | ROS integration |
| **Koku API (W)** | PostgreSQL (koku) | 5432 | TCP | Database writes |
| **Koku API (W)** | Redis | 6379 | TCP | Cache writes |
| **Koku API (W)** | Kafka | 29092 | TCP | Produce/consume messages |
| **Koku API (W)** | MinIO | 9000 | TCP | Write cost reports |
| **Koku API (W)** | Trino | 8080 | TCP | Query Parquet data |
| **Celery Beat** | Redis | 6379 | TCP | Task scheduling |
| **Celery Beat** | PostgreSQL (koku) | 5432 | TCP | Task metadata |
| **Celery Workers** | Redis | 6379 | TCP | Task queue (broker + results) |
| **Celery Workers** | PostgreSQL (koku) | 5432 | TCP | Process cost data |
| **Celery Workers** | Kafka | 29092 | TCP | Consume/produce messages |
| **Celery Workers** | MinIO | 9000 | TCP | Read/write Parquet files |
| **Celery Workers** | Trino | 8080 | TCP | Query/create Parquet tables |
| **Trino Coordinator** | Hive Metastore | 9083 | TCP | Metadata queries |
| **Trino Coordinator** | MinIO | 9000 | TCP | Read Parquet files |
| **Trino Coordinator** | PostgreSQL (koku) | 5432 | TCP | PostgreSQL catalog |
| **Trino Worker** | Trino Coordinator | 8080 | TCP | Task coordination |
| **Trino Worker** | MinIO | 9000 | TCP | Read Parquet files |
| **Trino Worker** | PostgreSQL (koku) | 5432 | TCP | PostgreSQL catalog |
| **Hive Metastore** | PostgreSQL (metastore) | 5432 | TCP | Metadata storage |
| **Ingress** | Koku API (R) | 8000 | TCP | External API access |
| **Ingress** | Koku API (W) | 8000 | TCP | External API access |

### NetworkPolicy Structure (OpenShift Only)

All NetworkPolicies will be conditional:

```yaml
{{- if .Values.global.platform.openshift }}
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: {{ include "cost-mgmt.koku.api.name" . }}-reads
spec:
  # ... policy rules
{{- end }}
```

### Default Egress Policy

All pods get DNS egress by default:

```yaml
egress:
- to:
  - namespaceSelector:
      matchLabels:
        name: openshift-dns
  ports:
  - protocol: UDP
    port: 53
```

---

## values-koku.yaml Structure

**Full file**: See `cost-management-onprem/values-koku.yaml` (created in Phase 1)

### Key Sections

```yaml
# Extend PR #27's costManagement placeholder
costManagement:
  enabled: true

  api:
    image:
      repository: quay.io/project-koku/koku
      tag: "latest"

    reads:
      enabled: true
      replicas: 2
      resources: { ... }

    writes:
      enabled: true
      replicas: 1
      resources: { ... }

  celery:
    beat: { ... }
    workers:
      default: { ... }
      priority: { ... }
      # ... 13 workers total

  database:
    name: koku
    storage:
      size: 20Gi

# New section for Trino
trino:
  enabled: true
  profile: minimal  # minimal, dev, or production

  coordinator: { ... }
  worker: { ... }
  metastore: { ... }
```

---

## Template Files

### Directory Structure

```
cost-management-onprem/templates/
â”œâ”€â”€ cost-management/          # NEW: 16 templates
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ deployment-reads.yaml
â”‚   â”‚   â”œâ”€â”€ deployment-writes.yaml
â”‚   â”‚   â”œâ”€â”€ service.yaml
â”‚   â”‚   â””â”€â”€ networkpolicy.yaml
â”‚   â”œâ”€â”€ celery/
â”‚   â”‚   â”œâ”€â”€ deployment-beat.yaml
â”‚   â”‚   â”œâ”€â”€ deployment-worker-default.yaml
â”‚   â”‚   â”œâ”€â”€ deployment-worker-priority.yaml
â”‚   â”‚   â”œâ”€â”€ deployment-worker-refresh.yaml
â”‚   â”‚   â”œâ”€â”€ deployment-worker-summary.yaml
â”‚   â”‚   â”œâ”€â”€ deployment-worker-hcs.yaml
â”‚   â”‚   â”œâ”€â”€ deployment-worker-priority-xl.yaml
â”‚   â”‚   â”œâ”€â”€ deployment-worker-refresh-xl.yaml
â”‚   â”‚   â”œâ”€â”€ deployment-worker-summary-xl.yaml
â”‚   â”‚   â”œâ”€â”€ deployment-worker-priority-penalty.yaml
â”‚   â”‚   â”œâ”€â”€ deployment-worker-refresh-penalty.yaml
â”‚   â”‚   â”œâ”€â”€ deployment-worker-summary-penalty.yaml
â”‚   â”‚   â”œâ”€â”€ networkpolicy-beat.yaml
â”‚   â”‚   â””â”€â”€ networkpolicy-workers.yaml
â”‚   â”œâ”€â”€ database/
â”‚   â”‚   â”œâ”€â”€ statefulset.yaml
â”‚   â”‚   â”œâ”€â”€ service.yaml
â”‚   â”‚   â””â”€â”€ networkpolicy.yaml
â”‚   â”œâ”€â”€ secrets/
â”‚   â”‚   â”œâ”€â”€ django-secret.yaml
â”‚   â”‚   â””â”€â”€ database-credentials.yaml
â”‚   â””â”€â”€ monitoring/
â”‚       â””â”€â”€ servicemonitor.yaml
â”œâ”€â”€ trino/                    # NEW: 7 templates
â”‚   â”œâ”€â”€ coordinator/
â”‚   â”‚   â”œâ”€â”€ statefulset.yaml
â”‚   â”‚   â”œâ”€â”€ service.yaml
â”‚   â”‚   â”œâ”€â”€ configmap.yaml
â”‚   â”‚   â””â”€â”€ networkpolicy.yaml
â”‚   â”œâ”€â”€ worker/
â”‚   â”‚   â”œâ”€â”€ deployment.yaml
â”‚   â”‚   â”œâ”€â”€ configmap.yaml
â”‚   â”‚   â””â”€â”€ networkpolicy.yaml
â”‚   â”œâ”€â”€ metastore/
â”‚   â”‚   â”œâ”€â”€ deployment.yaml
â”‚   â”‚   â”œâ”€â”€ service.yaml
â”‚   â”‚   â”œâ”€â”€ configmap.yaml
â”‚   â”‚   â”œâ”€â”€ networkpolicy.yaml
â”‚   â”‚   â””â”€â”€ database/
â”‚   â”‚       â”œâ”€â”€ statefulset.yaml
â”‚   â”‚       â”œâ”€â”€ service.yaml
â”‚   â”‚       â””â”€â”€ networkpolicy.yaml
â”‚   â””â”€â”€ monitoring/
â”‚       â””â”€â”€ servicemonitor.yaml
â””â”€â”€ _helpers-koku.tpl         # NEW: Koku helpers
```

---

## Service Communication Matrix

### Comprehensive View

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Service Communication Paths (27 total)                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                    â”‚
â”‚  Koku API (Reads)                                                â”‚
â”‚    â”œâ”€â–º PostgreSQL (koku):5432                                    â”‚
â”‚    â”œâ”€â–º Redis:6379                                                â”‚
â”‚    â”œâ”€â–º Kafka:29092                                               â”‚
â”‚    â”œâ”€â–º MinIO:9000                                                â”‚
â”‚    â”œâ”€â–º Trino:8080                                                â”‚
â”‚    â””â”€â–º ROS API:8000                                              â”‚
â”‚                                                                    â”‚
â”‚  Koku API (Writes)                                               â”‚
â”‚    â”œâ”€â–º PostgreSQL (koku):5432                                    â”‚
â”‚    â”œâ”€â–º Redis:6379                                                â”‚
â”‚    â”œâ”€â–º Kafka:29092                                               â”‚
â”‚    â”œâ”€â–º MinIO:9000                                                â”‚
â”‚    â””â”€â–º Trino:8080                                                â”‚
â”‚                                                                    â”‚
â”‚  Celery Beat                                                     â”‚
â”‚    â”œâ”€â–º Redis:6379                                                â”‚
â”‚    â””â”€â–º PostgreSQL (koku):5432                                    â”‚
â”‚                                                                    â”‚
â”‚  Celery Workers (13 types)                                       â”‚
â”‚    â”œâ”€â–º Redis:6379                                                â”‚
â”‚    â”œâ”€â–º PostgreSQL (koku):5432                                    â”‚
â”‚    â”œâ”€â–º Kafka:29092                                               â”‚
â”‚    â”œâ”€â–º MinIO:9000                                                â”‚
â”‚    â””â”€â–º Trino:8080                                                â”‚
â”‚                                                                    â”‚
â”‚  Trino Coordinator                                               â”‚
â”‚    â”œâ”€â–º Hive Metastore:9083                                       â”‚
â”‚    â”œâ”€â–º MinIO:9000                                                â”‚
â”‚    â””â”€â–º PostgreSQL (koku):5432                                    â”‚
â”‚                                                                    â”‚
â”‚  Trino Workers                                                   â”‚
â”‚    â”œâ”€â–º Trino Coordinator:8080                                    â”‚
â”‚    â”œâ”€â–º MinIO:9000                                                â”‚
â”‚    â””â”€â–º PostgreSQL (koku):5432                                    â”‚
â”‚                                                                    â”‚
â”‚  Hive Metastore                                                  â”‚
â”‚    â””â”€â–º PostgreSQL (metastore):5432                               â”‚
â”‚                                                                    â”‚
â”‚  Ingress (insights-ingress-go)                                   â”‚
â”‚    â”œâ”€â–º Koku API (Reads):8000                                     â”‚
â”‚    â”œâ”€â–º Koku API (Writes):8000                                    â”‚
â”‚    â””â”€â–º ROS API:8000                                              â”‚
â”‚                                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Resource Requirements

### Minimal Profile (Dev/Integration Testing)

| Component | Pods | CPU Request | Memory Request | CPU Limit | Memory Limit | Storage |
|-----------|------|-------------|----------------|-----------|--------------|---------|
| **Koku Services** | | | | | | |
| Koku API (Reads) | 2 | 600m | 1000Mi | 1200m | 2000Mi | - |
| Koku API (Writes) | 1 | 300m | 500Mi | 600m | 1000Mi | - |
| Celery Beat | 1 | 100m | 200Mi | 200m | 400Mi | - |
| Celery Workers (13) | 13 | 1300m | 2600Mi | 2600m | 5200Mi | - |
| Koku PostgreSQL | 1 | 500m | 1000Mi | 1000m | 2000Mi | 20Gi |
| **Koku Subtotal** | **18** | **2.8 cores** | **~5.3 GB** | **5.6 cores** | **~10.6 GB** | **20Gi** |
| | | | | | | |
| **Trino Services** | | | | | | |
| Trino Coordinator | 1 | 250m | 1000Mi | 500m | 2000Mi | 5Gi |
| Trino Worker | 1 | 250m | 1000Mi | 500m | 2000Mi | 5Gi |
| Hive Metastore | 1 | 100m | 256Mi | 250m | 512Mi | - |
| Metastore DB | 1 | 50m | 128Mi | 100m | 256Mi | 2Gi |
| **Trino Subtotal** | **4** | **0.65 cores** | **~2.4 GB** | **1.35 cores** | **~4.8 GB** | **12Gi** |
| | | | | | | |
| **TOTAL (Koku + Trino)** | **22** | **~3.5 cores** | **~7.7 GB** | **~7 cores** | **~15.4 GB** | **32Gi** |

### With Existing ROS

| Component | Pods | CPU Request | Memory Request | Storage |
|-----------|------|-------------|----------------|---------|
| Existing ROS | ~13 | ~4 cores | ~10 GB | ~40Gi |
| **Koku + Trino** | **22** | **~3.5 cores** | **~7.7 GB** | **32Gi** |
| **TOTAL** | **~35** | **~7.5 cores** | **~17.7 GB** | **~72Gi** |

### Cluster Capacity (stress.parodos.dev)

```
Available Resources:
- Control plane (3 nodes): 30 cores, 96GB RAM
- Workers (3 nodes): 48 cores, 96GB RAM
- Total: 78 cores, 192GB RAM

Our Requirements: 7.5 cores, 17.7GB RAM

Status: âœ… WELL WITHIN CAPACITY (10% CPU, 9% RAM)
```

---

## Testing Strategy

### Unit Testing
- Helm template rendering tests
- Values validation tests
- Helper function tests

### Integration Testing

1. **Phase 1: Basic Deployment**
   - Deploy with minimal values
   - Verify all pods start
   - Verify all services are created
   - Check pod logs for errors

2. **Phase 2: Connectivity Testing**
   - Test Koku API â†’ PostgreSQL
   - Test Koku API â†’ Redis
   - Test Koku API â†’ Kafka
   - Test Koku API â†’ MinIO
   - Test Koku API â†’ Trino
   - Test Celery Beat â†’ Redis
   - Test Celery Workers â†’ All dependencies
   - Test Trino â†’ Metastore
   - Test Trino â†’ MinIO
   - Test Trino â†’ PostgreSQL

3. **Phase 3: Functional Testing**
   - Create test source via API
   - Upload sample cost data to MinIO
   - Trigger Celery worker to process data
   - Query data via Koku API
   - Query Parquet data via Trino

4. **Phase 4: NetworkPolicy Testing** (OCP only)
   - Deploy with NetworkPolicies enabled
   - Verify allowed traffic works
   - Verify blocked traffic is blocked
   - Test with `kubectl exec` + `curl`

5. **Phase 5: Resource Testing**
   - Monitor resource usage under load
   - Verify resource limits are respected
   - Check for OOMKilled pods
   - Optimize resource requests/limits

### Test Commands

```bash
# 1. Deploy
helm install cost-mgmt ./cost-management-onprem \
  --namespace cost-mgmt \
  --create-namespace \
  -f cost-management-onprem/values.yaml \
  -f cost-management-onprem/values-koku.yaml

# 2. Verify pods
kubectl get pods -n cost-mgmt -l app.kubernetes.io/component=cost-management-api
kubectl get pods -n cost-mgmt -l app.kubernetes.io/component=cost-management-celery

# 3. Check logs
kubectl logs -n cost-mgmt -l api-type=reads --tail=50
kubectl logs -n cost-mgmt -l api-type=writes --tail=50
kubectl logs -n cost-mgmt deployment/cost-mgmt-celery-beat --tail=50

# 4. Test API
kubectl port-forward -n cost-mgmt svc/cost-mgmt-koku-api 8000:8000
curl http://localhost:8000/api/cost-management/v1/status/

# 5. Test Trino
kubectl port-forward -n cost-mgmt svc/cost-mgmt-trino-coordinator 8080:8080
curl http://localhost:8080/ui/

# 6. Test NetworkPolicies (if OCP)
kubectl exec -n cost-mgmt deployment/cost-mgmt-koku-api-reads -- \
  curl -v http://cost-mgmt-postgresql:5432
```

---

## Risks & Mitigations

### High Risks

| Risk | Impact | Probability | Mitigation |
|------|--------|-------------|------------|
| **PR #27 merge delays** | Blocks our start | Medium | Monitor PR actively, prepare in parallel |
| **Insufficient cluster resources** | Deployment fails | Low | Already validated (78 cores available) |
| **NetworkPolicy too restrictive** | Services can't communicate | Medium | Test incrementally, document all paths |
| **Trino resource starvation** | Queries fail | Medium | Use minimal profile, monitor closely |

### Medium Risks

| Risk | Impact | Probability | Mitigation |
|------|--------|-------------|------------|
| **Helper function incompatibility** | Template rendering fails | Low | Use PR #27's helpers, test early |
| **Database migration issues** | Data loss | Low | Use init containers, validate schema |
| **Secret generation conflicts** | Upgrade breaks | Low | Use `lookup` function for stability |
| **Celery worker queue misconfiguration** | Tasks not processed | Medium | Follow ClowdApp exactly, test each queue |

### Low Risks

| Risk | Impact | Probability | Mitigation |
|------|--------|-------------|------------|
| **Documentation drift** | Confusion | Low | Update docs continuously |
| **Linting errors** | PR blocked | Low | Run linters frequently |
| **Values file merge conflicts** | Manual resolution needed | Low | Separate `values-koku.yaml` avoids this |

---

## Success Criteria

### Must Have (P0)

- âœ… All 23 templates render without errors
- âœ… All pods start and reach `Running` state
- âœ… Koku API responds to `/api/cost-management/v1/status/`
- âœ… Celery workers connect to Redis broker
- âœ… Trino coordinator accepts connections
- âœ… NetworkPolicies allow required traffic (OCP)

### Should Have (P1)

- âœ… Koku API can query PostgreSQL
- âœ… Koku API can read from MinIO
- âœ… Koku API can query Trino
- âœ… Celery workers process tasks
- âœ… Trino can query Parquet files from MinIO
- âœ… Trino can query PostgreSQL catalog

### Nice to Have (P2)

- âœ… Full end-to-end cost data processing
- âœ… Comprehensive documentation
- âœ… Helm tests for smoke testing
- âœ… Resource profiles (minimal, dev, production)

---

## Next Steps

### Immediate (After Review Approval)

1. **Create feature branch**: `feature/koku-integration-post-pr27`
2. **Start Phase 1**: Create `values-koku.yaml` and `_helpers-koku.tpl`
3. **Monitor PR #27**: Watch for merge to main
4. **Parallel work**: Prepare templates while waiting for #27

### After PR #27 Merges

1. **Rebase** our branch on latest main
2. **Test** with both values files
3. **Continue** with Phase 2 (Koku Core Services)
4. **Daily check-ins** to track progress

---

## Questions for Review

### Critical Questions

1. **Approach Confirmation**: Is the separate `values-koku.yaml` approach acceptable?
2. **Resource Allocation**: Are minimal resources (3.5 cores, 7.7GB RAM) acceptable for dev/integration testing?
3. **NetworkPolicy Scope**: Should we implement all 15 NetworkPolicies, or start with a subset?
4. **Trino Profile**: Should we use minimal profile (1 worker) or scale up to dev profile (2-3 workers)?
5. **Timeline**: Is 4 weeks (20 working days) acceptable, or do we need faster delivery?

### Optional Questions

6. **Testing Environment**: When can we deploy to `stress.parodos.dev`?
7. **Secrets Management**: Should we use auto-generation or external secrets?
8. **Database Persistence**: Should Koku database be persistent or ephemeral for dev?
9. **Monitoring Integration**: Should we add custom Prometheus metrics for Koku?
10. **Backup Strategy**: Do we need backup/restore for Koku database?

---

## Summary

**Status**: ğŸ“‹ **READY FOR REVIEW**
**Confidence**: ğŸŸ¢ **95% HIGH CONFIDENCE**

### What We Have

- âœ… Complete implementation plan (6 phases, 28 days)
- âœ… Detailed source code analysis (Koku ports, protocols, dependencies)
- âœ… NetworkPolicy requirements (27 communication paths documented)
- âœ… Resource requirements validated (cluster has capacity)
- âœ… Risk assessment with mitigations
- âœ… Success criteria defined

### What We Need

- ğŸ” **Your review and approval** of this plan
- â³ **PR #27 to merge** (already in progress)
- ğŸš€ **Go-ahead to start implementation**

### Timeline

```
Week 1 (Days 1-7):   Setup + Koku Core Services
Week 2 (Days 8-14):  Celery Workers + Start Trino
Week 3 (Days 15-21): Finish Trino + NetworkPolicies + Integration
Week 4 (Days 22-28): Testing + Documentation + PR
```

### Next Action

**Awaiting your review and feedback on this plan.** Once approved, we'll:
1. Create feature branch
2. Start with Phase 1 (values-koku.yaml)
3. Monitor PR #27 for merge
4. Proceed with implementation

---

**Document Version**: 1.0
**Date**: November 6, 2025
**Author**: AI Assistant (Claude)
**Status**: ğŸ“‹ **READY FOR REVIEW**

