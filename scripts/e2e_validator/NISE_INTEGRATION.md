# Nise Integration in E2E Validation

## Overview

**Nise** (synthetic data generator) runs in **Phase 4** to create predictable test data for financial validation. This enables comprehensive E2E testing of cost calculations, aggregations, and reporting.

## When Nise Runs

```
E2E Validation Flow:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Phase 1: Preflight Checks
  âœ“ Verify cluster health
  âœ“ Check pod status

Phase 2: Database Migrations
  âœ“ Apply schema updates
  âœ“ Create Hive prerequisites

Phase 3: Provider Setup
  âœ“ Create or verify provider
  âœ“ Configure data source

Phase 4: Generate & Upload Test Data   â† **NISE RUNS HERE** ğŸ¯
  â”œâ”€ 4a. Generate scenarios with Nise
  â”‚    â”œâ”€ basic_compute (EC2 instances)
  â”‚    â”œâ”€ storage_costs (EBS, S3)
  â”‚    â”œâ”€ tagged_resources (cost allocation)
  â”‚    â”œâ”€ multi_account (org structure)
  â”‚    â””â”€ daily_variation (trending)
  â”‚
  â”œâ”€ 4b. Upload data to S3
  â”‚    â”œâ”€ CSV files (gzipped CUR format)
  â”‚    â””â”€ Manifest JSON
  â”‚
  â””â”€ 4c. Verify upload
       â””â”€ Confirm S3 object count

Phase 5: Trigger MASU Processing
  âœ“ Send Celery task to download data
  âœ“ Get task ID

Phase 6: Monitor Processing
  âœ“ Wait for manifest in database
  âœ“ Verify data processing started

Phase 7: Verify Trino Tables
  âœ“ Check Hive schema creation
  âœ“ Verify table count

Phase 8: Run IQE Test Suite   â† **VALIDATES NISE SCENARIOS** ğŸ¯
  â”œâ”€ Cost calculation tests
  â”‚    â””â”€ Verify expected costs match Nise scenarios
  â”‚
  â”œâ”€ Aggregation tests
  â”‚    â””â”€ Sum by service, region, account
  â”‚
  â”œâ”€ Tag filtering tests
  â”‚    â””â”€ Filter by tags from Nise scenarios
  â”‚
  â”œâ”€ Trending tests
  â”‚    â””â”€ Daily/monthly cost trends
  â”‚
  â””â”€ Multi-account tests
       â””â”€ Organization hierarchy costs

Phase 9: Summary
  âœ“ Report pass/fail status
  âœ“ Show scenario validation results
```

## Test Scenarios

### Predefined Scenarios

Each scenario generates **predictable, deterministic data** for specific test cases:

#### 1. `basic_compute`
**Purpose**: Test basic EC2 cost calculations

**Generated Data**:
- 5 EC2 instances (t3.medium, t3.large)
- 24 hours/day usage
- Known hourly rates
- **Expected Total Cost**: $1,000.00

**IQE Tests**:
```python
def test_basic_compute_costs(api_client):
    """Verify basic EC2 costs match Nise scenario"""
    response = api_client.get_costs(
        filter='service:AmazonEC2',
        group_by='instance_type'
    )
    
    assert response['total'] == 1000.00
    assert 't3.medium' in response['data']
    assert 't3.large' in response['data']
```

#### 2. `storage_costs`
**Purpose**: Test storage cost calculations

**Generated Data**:
- EBS volumes (gp3, 100GB each)
- S3 storage (standard tier)
- **Expected Total Cost**: $500.00

**IQE Tests**:
```python
def test_storage_costs(api_client):
    """Verify EBS + S3 costs"""
    response = api_client.get_costs(
        filter='service:AmazonEC2,AmazonS3',
        group_by='service'
    )
    
    assert response['total'] == 500.00
```

#### 3. `tagged_resources`
**Purpose**: Test cost allocation tags

**Generated Data**:
- Resources with tags:
  - `environment:production`
  - `environment:development`
  - `app:web-server`
  - `app:database`
- **Expected Total Cost**: $750.00

**IQE Tests**:
```python
def test_tag_filtering(api_client):
    """Verify tag-based cost filtering"""
    prod_costs = api_client.get_costs(
        filter='tag:environment=production'
    )
    dev_costs = api_client.get_costs(
        filter='tag:environment=development'
    )
    
    assert prod_costs['total'] + dev_costs['total'] == 750.00
```

#### 4. `multi_account`
**Purpose**: Test organization hierarchy

**Generated Data**:
- 2 AWS accounts
- Different cost patterns per account
- **Expected Total Cost**: $2,000.00

**IQE Tests**:
```python
def test_multi_account_costs(api_client):
    """Verify org-level aggregation"""
    response = api_client.get_costs(
        group_by='account'
    )
    
    assert len(response['accounts']) == 2
    assert response['total'] == 2000.00
```

#### 5. `daily_variation`
**Purpose**: Test trending and forecasting

**Generated Data**:
- Daily costs with known pattern
- Increasing trend: $40/day â†’ $60/day
- **Expected Total Cost**: $1,500.00 (30 days)

**IQE Tests**:
```python
def test_cost_trending(api_client):
    """Verify daily cost trends"""
    response = api_client.get_costs(
        group_by='date',
        time_scope='monthly'
    )
    
    # Verify increasing trend
    daily_costs = [d['cost'] for d in response['data']]
    assert daily_costs[-1] > daily_costs[0]
```

## Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Nise CLI  â”‚  Generate synthetic CUR data
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ Writes to local temp directory
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Local Files:                           â”‚
â”‚  â”œâ”€ 20251101-20251130/                 â”‚
â”‚  â”‚  â”œâ”€ test-report-1.csv.gz            â”‚ â† CSV data
â”‚  â”‚  â”œâ”€ test-report-2.csv.gz            â”‚
â”‚  â”‚  â””â”€ test-report-Manifest.json       â”‚ â† Manifest
â”‚  â””â”€ ...                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â”‚ Upload via boto3
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ S3 Bucket (MinIO/ODF):                 â”‚
â”‚  cost-data/                            â”‚
â”‚  â””â”€ test-report/                       â”‚
â”‚     â””â”€ 20251101-20251130/              â”‚
â”‚        â”œâ”€ test-report-1.csv.gz         â”‚
â”‚        â”œâ”€ test-report-2.csv.gz         â”‚
â”‚        â””â”€ test-report-Manifest.json    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â”‚ MASU downloads
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MASU Processing:                       â”‚
â”‚  1. Download from S3                   â”‚
â”‚  2. Parse CSV                          â”‚
â”‚  3. Write to Postgres                  â”‚
â”‚  4. Create Hive tables                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PostgreSQL:                            â”‚
â”‚  â”œâ”€ reporting_awscostentrylineitem     â”‚
â”‚  â”œâ”€ reporting_awscostentry_daily_sum   â”‚
â”‚  â””â”€ ...                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â”‚ Trino queries
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Trino Tables:                          â”‚
â”‚  hive.org1234567.aws_line_items        â”‚
â”‚  hive.org1234567.reporting_awscost...  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â”‚ API queries
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Koku API:                              â”‚
â”‚  /api/cost-management/v1/reports/...   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â”‚ IQE tests validate
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ IQE Test Suite:                        â”‚
â”‚  âœ“ test_basic_compute_costs()          â”‚
â”‚  âœ“ test_storage_costs()                â”‚
â”‚  âœ“ test_tag_filtering()                â”‚
â”‚  âœ“ test_multi_account_costs()          â”‚
â”‚  âœ“ test_cost_trending()                â”‚
â”‚  ...90+ tests total                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Usage Examples

### Full Scenario Suite (CI/CD)
```bash
# Run complete E2E with all scenarios
python3 -m e2e_validator.cli \
    --namespace cost-mgmt \
    --scenarios basic_compute,storage_costs,tagged_resources,multi_account,daily_variation

# This will:
# 1. Generate 5 scenarios with Nise (~2 minutes)
# 2. Upload to S3 (~30 seconds)
# 3. Trigger MASU processing (~3 minutes)
# 4. Run 90+ IQE tests (~5 minutes)
# Total time: ~11 minutes
```

### Quick Single Scenario (Development)
```bash
# Fast iteration with basic scenario only
python3 -m e2e_validator.cli \
    --namespace cost-mgmt \
    --quick

# This will:
# 1. Generate basic_compute only (~30 seconds)
# 2. Upload to S3 (~10 seconds)
# 3. Trigger MASU processing (~2 minutes)
# 4. Run subset of tests (~2 minutes)
# Total time: ~5 minutes
```

### Custom Scenario
```python
from e2e_validator.clients.nise import NiseClient
from datetime import datetime, timedelta

nise = NiseClient()

# Generate custom data
end_date = datetime.now()
start_date = end_date - timedelta(days=7)

data_path = nise.generate_aws_cur(
    start_date=start_date,
    end_date=end_date,
    account_id='999888777666',
    tags=['team:platform', 'env:staging']
)

# Upload and validate...
```

## Scenario Validation

After tests run, scenarios are validated:

```python
from e2e_validator.clients.nise import NiseScenarioValidator

validator = NiseScenarioValidator(db_client)

# Validate each scenario
for scenario in ['basic_compute', 'storage_costs', 'tagged_resources']:
    result = validator.validate_scenario_costs(
        scenario_name=scenario,
        tolerance=0.01  # 1% tolerance
    )
    
    print(f"{scenario}:")
    print(f"  Expected: ${result['expected_cost']:.2f}")
    print(f"  Actual:   ${result['actual_cost']:.2f}")
    print(f"  Variance: {result['variance']*100:.2f}%")
    print(f"  Status:   {'PASS' if result['passed'] else 'FAIL'}")
```

**Example Output**:
```
basic_compute:
  Expected: $1000.00
  Actual:   $1002.15
  Variance: 0.21%
  Status:   PASS âœ“

storage_costs:
  Expected: $500.00
  Actual:   $499.87
  Variance: 0.03%
  Status:   PASS âœ“

tagged_resources:
  Expected: $750.00
  Actual:   $750.00
  Variance: 0.00%
  Status:   PASS âœ“
```

## Benefits of Nise Integration

1. **Deterministic Testing**: Known inputs â†’ predictable outputs
2. **Comprehensive Coverage**: Multiple scenarios test different features
3. **Fast Iteration**: Generate fresh data in seconds
4. **Reproducible**: Same scenarios always produce same data
5. **Version Control**: Scenarios defined in code, not data files
6. **Isolated**: Each test run uses fresh data, no state pollution

## Next Steps

See the main CLI documentation for running the complete E2E suite with Nise scenarios.

