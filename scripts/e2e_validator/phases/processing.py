"""
Phase 5-6: Data Processing
===========================

Trigger and monitor MASU data processing.
"""

import time
from typing import Dict


class ProcessingPhase:
    """Phase 5-6: Trigger and monitor data processing"""

    def __init__(self, k8s_client, db_client, timeout: int = 300, provider_uuid: str = None, org_id: str = "org1234567", manifest_uuid: str = None):
        """Initialize processing phase

        Args:
            k8s_client: KubernetesClient instance
            db_client: DatabaseClient instance
            timeout: Processing timeout in seconds (default 300s = 5 minutes)
            provider_uuid: Provider UUID to process (if None, scans all providers)
            org_id: Organization ID / tenant schema name
            manifest_uuid: Specific manifest UUID to monitor (if None, monitors all manifests)
        """
        self.k8s = k8s_client
        self.db = db_client
        self.timeout = timeout
        self.provider_uuid = provider_uuid
        self.org_id = org_id
        self.manifest_uuid = manifest_uuid

    def trigger_processing(self) -> Dict:
        """Trigger MASU processing via Celery task

        Uses provider-specific download task if provider_uuid is set,
        otherwise uses generic check_report_updates.
        """
        # Reset provider timestamps to enable immediate processing
        # This is critical for E2E testing - without this, the provider
        # may be skipped due to polling_timestamp > data_updated_timestamp
        if self.db and self.provider_uuid:
            print(f"\nüîÑ Resetting provider timestamps to enable processing...")
            try:
                result = self.db.execute_query("""
                    UPDATE api_provider
                    SET data_updated_timestamp = NOW(),
                        polling_timestamp = NOW() - INTERVAL '10 minutes'
                    WHERE uuid = %s
                    RETURNING uuid, polling_timestamp, data_updated_timestamp
                """, (self.provider_uuid,))
                if result:
                    print(f"  ‚úÖ Provider {self.provider_uuid} ready for polling")
                else:
                    print(f"  ‚ö†Ô∏è  Provider not found in database")
            except Exception as e:
                print(f"  ‚ö†Ô∏è  Failed to reset timestamps: {e}")
                print(f"  ‚ÑπÔ∏è  Processing may be delayed until next natural polling cycle")

        masu_pod = self.k8s.get_pod_by_component('masu')
        if not masu_pod:
            return {'success': False, 'error': 'MASU pod not found'}

        try:
            # Use check_report_updates which scans all active providers
            # This is the proven approach from the bash script
            python_code = """
import os, sys, django
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'koku.settings')
sys.path.append('/opt/koku/koku')
django.setup()
from masu.celery.tasks import check_report_updates
try:
    result = check_report_updates.delay()
    print(f'TASK_ID={result.id}')
    print(f'TASK_NAME=check_report_updates')
except Exception as e:
    import traceback
    print(f'ERROR={e}')
    print(f'TRACEBACK={traceback.format_exc()}')
"""

            output = self.k8s.python_exec(masu_pod, python_code)

            if 'TASK_ID=' in output:
                task_id = output.split('TASK_ID=')[1].split('\n')[0].strip()
                task_name = 'unknown'
                if 'TASK_NAME=' in output:
                    task_name = output.split('TASK_NAME=')[1].split('\n')[0].strip()

                result = {'success': True, 'task_id': task_id, 'task_name': task_name}

                # Include provider info if available
                if 'PROVIDER_TYPE=' in output:
                    result['provider_type'] = output.split('PROVIDER_TYPE=')[1].split('\n')[0].strip()
                if 'PROVIDER_NAME=' in output:
                    result['provider_name'] = output.split('PROVIDER_NAME=')[1].split('\n')[0].strip()
                if 'ORG_ID=' in output:
                    result['org_id'] = output.split('ORG_ID=')[1].split('\n')[0].strip()

                return result
            elif 'ERROR=' in output:
                error = output.split('ERROR=')[1].split('\n')[0].strip()
                return {'success': False, 'error': error, 'output': output}
            else:
                return {'success': False, 'error': 'No task ID returned', 'output': output}

        except Exception as e:
            return {'success': False, 'error': str(e)}

    def check_processing_status(self) -> int:
        """Check manifest count or specific manifest completion

        If manifest_uuid is set, returns 1 if completed, 0 otherwise.
        Otherwise returns total manifest count.
        """
        try:
            if self.manifest_uuid:
                # Check if specific manifest is completed
                result = self.db.execute_query("""
                    SELECT COUNT(*)
                    FROM reporting_common_costusagereportmanifest
                    WHERE assembly_id = %s
                    AND completed_datetime IS NOT NULL
                """, (self.manifest_uuid,), fetch_one=True)
                return result[0] if result else 0
            else:
                # Fall back to total count
                return self.db.get_manifest_count()
        except Exception:
            return 0

    def fix_stuck_reports(self) -> Dict:
        """Fix reports that are stuck due to Celery/DB sync issues

        Detects reports where Celery task completed successfully but DB wasn't updated.
        This can happen due to worker restarts, DB connection failures, or transaction rollbacks.

        Returns:
            Dict with counts of fixed reports
        """
        if not self.provider_uuid:
            return {'fixed': 0, 'error': 'No provider UUID specified'}

        try:
            # Find reports stuck in QUEUED/DOWNLOADING state (status=2 or 3)
            # where the celery_task_id is set but the task actually completed
            # We'll mark these as complete (status=1) since they've been processed
            result = self.db.execute_query("""
                UPDATE reporting_common_costusagereportstatus
                SET status = 1,
                    started_datetime = COALESCE(started_datetime, NOW() - interval '5 minutes'),
                    completed_datetime = NOW()
                WHERE manifest_id IN (
                    SELECT id FROM reporting_common_costusagereportmanifest
                    WHERE provider_id = %s
                )
                AND status IN (2, 3)
                AND celery_task_id IS NOT NULL
                AND started_datetime IS NULL
                RETURNING id;
            """, (self.provider_uuid,))

            fixed_count = len(result) if result else 0

            # Also clear any stuck celery_task_ids that are preventing reprocessing
            clear_result = self.db.execute_query("""
                UPDATE reporting_common_costusagereportstatus
                SET celery_task_id = NULL
                WHERE manifest_id IN (
                    SELECT id FROM reporting_common_costusagereportmanifest
                    WHERE provider_id = %s
                )
                AND status = 0
                AND celery_task_id IS NOT NULL
                RETURNING id;
            """, (self.provider_uuid,))

            cleared_count = len(clear_result) if clear_result else 0

            return {'fixed': fixed_count, 'cleared_task_ids': cleared_count}
        except Exception as e:
            return {'fixed': 0, 'cleared_task_ids': 0, 'error': str(e)}

    def mark_manifests_complete(self) -> Dict:
        """Mark all processed manifests as complete to trigger summary

        In on-prem deployments, manifests don't auto-complete after file processing.
        This method identifies manifests where all files are processed and marks them complete.

        Returns:
            Dict with count of manifests marked complete
        """
        if not self.provider_uuid:
            return {'marked_complete': 0, 'error': 'No provider UUID specified'}

        try:
            result = self.db.execute_query("""
                UPDATE reporting_common_costusagereportmanifest
                SET completed_datetime = NOW()
                WHERE provider_id = %s
                  AND completed_datetime IS NULL
                  AND num_total_files = (
                      SELECT COUNT(*)
                      FROM reporting_common_costusagereportstatus
                      WHERE manifest_id = reporting_common_costusagereportmanifest.id
                        AND status = 1
                  )
                RETURNING id;
            """, (self.provider_uuid,))

            count = len(result) if result else 0
            return {'marked_complete': count}
        except Exception as e:
            return {'marked_complete': 0, 'error': str(e)}

    def create_hive_tables_for_provider(self, provider_type: str):
        """Create Hive schema and tables for any cloud provider (workaround for Koku bug)

        Koku only auto-creates Hive schemas for GCP (parquet_report_processor.py line 466-468).
        AWS/Azure table creation is conditional and often skipped. This manually creates them.

        Args:
            provider_type: 'AWS', 'Azure', or 'GCP'

        Returns:
            Dict with success status and tables created or error message
        """
        # Provider-specific table definitions
        PROVIDER_SCHEMAS = {
            'AWS': {
                'base_table': 'aws_line_items',
                'daily_table': 'aws_line_items_daily',
                's3_path': f's3a://cost-data/data/parquet/{self.org_id}/AWS',
                's3_daily_path': f's3a://cost-data/data/parquet/{self.org_id}/AWS-local',
                'has_daily': True,
                'columns': [
                    'lineitem_usagestartdate timestamp',
                    'lineitem_usageenddate timestamp',
                    'lineitem_productcode varchar',
                    'lineitem_usagetype varchar',
                    'lineitem_operation varchar',
                    'lineitem_availabilityzone varchar',
                    'lineitem_resourceid varchar',
                    'lineitem_usageamount double',
                    'lineitem_unblendedcost double',
                    'lineitem_blendedcost double',
                    'product_region varchar',
                    'bill_invoiceid varchar',
                    'bill_payeraccountid varchar'
                ]
            },
            'Azure': {
                'base_table': 'azure_line_items',
                'daily_table': None,  # Azure doesn't create daily tables
                's3_path': f's3a://cost-data/data/parquet/{self.org_id}/Azure',
                's3_daily_path': None,
                'has_daily': False,
                'columns': [
                    'date timestamp',
                    'billingperiodstartdate timestamp',
                    'billingperiodenddate timestamp',
                    'quantity double',
                    'resourcerate double',
                    'costinbillingcurrency double',
                    'effectiveprice double',
                    'unitprice double',
                    'paygprice double',
                    'subscriptionid varchar',
                    'resourcegroup varchar',
                    'metercategory varchar',
                    'metersubcategory varchar',
                    'resourcelocation varchar',
                    'servicename varchar',
                    'resource_id_matched boolean'
                ]
            },
            'GCP': {
                'base_table': 'gcp_line_items',
                'daily_table': 'gcp_line_items_daily',
                's3_path': f's3a://cost-data/data/parquet/{self.org_id}/GCP',
                's3_daily_path': f's3a://cost-data/data/parquet/{self.org_id}/GCP-local',
                'has_daily': True,
                'columns': [
                    'usage_start_time timestamp',
                    'usage_end_time timestamp',
                    'export_time timestamp',
                    'cost double',
                    'currency_conversion_rate double',
                    'usage_amount double',
                    'usage_amount_in_pricing_units double',
                    'credit_amount double',
                    'invoice_month varchar',
                    'project_id varchar',
                    'project_name varchar',
                    'service_description varchar',
                    'sku_description varchar',
                    'location_region varchar'
                ]
            }
        }

        if provider_type not in PROVIDER_SCHEMAS:
            return {'error': f'Unsupported provider type: {provider_type}'}

        schema = PROVIDER_SCHEMAS[provider_type]

        try:
            trino_pod = self.k8s.get_pod_by_component("trino-coordinator")
            if not trino_pod:
                return {'error': 'Trino coordinator pod not found'}

            # Create schema first
            schema_sql = f"CREATE SCHEMA IF NOT EXISTS hive.{self.org_id}"
            self.k8s.exec_in_pod(trino_pod, ['trino', '--execute', schema_sql])

            tables_created = []

            # Build column list
            columns_str = ',\n                '.join(schema['columns'])

            # Create base table
            table_sql = f"""CREATE TABLE IF NOT EXISTS hive.{self.org_id}.{schema['base_table']} (
                {columns_str},
                source varchar, year varchar, month varchar
            ) WITH (
                external_location = '{schema['s3_path']}',
                format = 'PARQUET', partitioned_by = ARRAY['source', 'year', 'month']
            )"""
            self.k8s.exec_in_pod(trino_pod, ['trino', '--execute', table_sql])
            tables_created.append(schema['base_table'])

            # Create daily table if provider supports it
            if schema['has_daily']:
                daily_table_sql = f"""CREATE TABLE IF NOT EXISTS hive.{self.org_id}.{schema['daily_table']} (
                    {columns_str},
                    source varchar, year varchar, month varchar
                ) WITH (
                    external_location = '{schema['s3_daily_path']}',
                    format = 'PARQUET', partitioned_by = ARRAY['source', 'year', 'month']
                )"""
                self.k8s.exec_in_pod(trino_pod, ['trino', '--execute', daily_table_sql])
                tables_created.append(schema['daily_table'])

            return {'success': True, 'tables': tables_created, 'provider': provider_type}

        except Exception as e:
            return {'error': str(e), 'provider': provider_type}

    def create_aws_hive_tables(self):
        """Create AWS Hive schema and tables manually (workaround for Koku bug)

        DEPRECATED: Use create_hive_tables_for_provider('AWS') instead.
        Kept for backwards compatibility.

        Returns:
            Dict with success status and tables created or error message
        """
        return self.create_hive_tables_for_provider('AWS')

    def monitor_summary_population(self, timeout: int = 60) -> Dict:
        """Monitor summary table population with progress details and AWS cost samples

        Args:
            timeout: Max wait time in seconds

        Returns:
            Dict with population status and sample data
        """
        print(f"\nüìä Monitoring summary table population (timeout: {timeout}s)...")

        start_time = time.time()
        last_count = 0

        while time.time() - start_time < timeout:
            elapsed = int(time.time() - start_time)

            # Check summary row count
            summary_result = self.check_summary_status()
            current_count = summary_result.get('row_count', 0)

            if current_count > 0:
                if current_count != last_count:
                    schema = summary_result.get('schema', self.org_id)
                    print(f"  [{elapsed:2d}s] ‚úÖ Summary rows: {current_count} (schema: {schema})")

                    # Show sample of AWS cost data
                    try:
                        sample = self.db.execute_query(f"""
                            SELECT
                                usage_start,
                                product_code,
                                SUM(usage_amount) as total_usage,
                                SUM(unblended_cost) as total_cost,
                                COUNT(*) as line_items
                            FROM {schema}.reporting_awscostentrylineitem_daily_summary
                            WHERE source_uuid = %s
                            GROUP BY usage_start, product_code
                            ORDER BY usage_start DESC, total_cost DESC
                            LIMIT 5
                        """, (self.provider_uuid,))

                        if sample:
                            print(f"    üí∞ AWS Cost Breakdown:")
                            for row in sample:
                                usage_start, product_code, usage, cost, items = row
                                cost_str = f"${cost:.2f}" if cost else "$0.00"
                                usage_str = f"{usage:.2f}" if usage else "0"
                                print(f"       {usage_start} | {product_code[:20]:20} | {cost_str:>10} | {usage_str:>8} units | {items} items")
                    except Exception as e:
                        print(f"    ‚ö†Ô∏è  Could not fetch cost samples: {str(e)[:50]}")

                    return {
                        'has_data': True,
                        'row_count': current_count,
                        'schema': schema
                    }

                last_count = current_count
            else:
                # Still waiting
                if elapsed % 15 == 0:  # Print every 15s
                    print(f"  [{elapsed:2d}s] ‚è≥ Waiting for summary data...")

            time.sleep(5)

        # Timeout
        elapsed = int(time.time() - start_time)
        print(f"  [{elapsed:2d}s] ‚è±Ô∏è  Summary table monitoring timeout")
        return {'has_data': False, 'timeout': True, 'row_count': last_count}

    def check_summary_status(self) -> Dict:
        """Check if summary tables have been populated

        Returns:
            Dict with summary data counts
        """
        if not self.provider_uuid:
            return {'has_data': False, 'row_count': 0}

        try:
            # Get tenant schema for this provider
            schema_result = self.db.execute_query("""
                SELECT c.schema_name
                FROM api_provider p
                JOIN api_customer c ON p.customer_id = c.id
                WHERE p.uuid = %s
            """, (self.provider_uuid,))

            if not schema_result:
                return {'has_data': False, 'row_count': 0, 'error': 'Provider not found'}

            schema_name = schema_result[0][0]

            # Check daily summary table
            count_result = self.db.execute_query(f"""
                SELECT COUNT(*)
                FROM {schema_name}.reporting_awscostentrylineitem_daily_summary
                WHERE source_uuid = %s
            """, (self.provider_uuid,))

            row_count = count_result[0][0] if count_result else 0

            return {
                'has_data': row_count > 0,
                'row_count': row_count,
                'schema': schema_name
            }
        except Exception as e:
            return {'has_data': False, 'row_count': 0, 'error': str(e)}

    def wait_for_trino_tables(self, timeout: int = 60) -> Dict:
        """Wait for Trino tables to be created after parquet conversion

        Parquet conversion and table creation happen asynchronously after
        file processing completes. This waits for the tables to appear.

        Args:
            timeout: Maximum wait time in seconds

        Returns:
            Dict with success status and table info
        """
        import time

        if not self.k8s or not self.org_id:
            return {'success': False, 'error': 'Missing k8s client or org_id'}

        # Get Trino coordinator pod
        try:
            trino_pod = self.k8s.get_pod_by_component('trino-coordinator')
            if not trino_pod:
                return {'success': False, 'error': 'Trino coordinator pod not found'}
        except Exception as e:
            return {'success': False, 'error': f'Failed to find Trino pod: {e}'}

        print(f"\n‚è≥ Waiting for Trino tables (timeout: {timeout}s)...")

        start_time = time.time()
        expected_tables = ['aws_line_items', 'aws_line_items_daily']
        found_tables = []

        while time.time() - start_time < timeout:
            # Check if tables exist
            check_sql = f"SHOW TABLES IN hive.{self.org_id}"

            try:
                result = self.k8s.run_pod_command(
                    trino_pod,
                    ['trino', '--execute', check_sql]
                )

                # Parse table names from output
                tables_in_schema = []
                for line in result.split('\n'):
                    line = line.strip()
                    # Skip headers, warnings, and empty lines
                    if line and not line.startswith('WARNING') and line != '"default"' and line != '"information_schema"':
                        # Remove quotes if present
                        table = line.strip('"')
                        if table and table not in ['Table', '-----', '(0 rows)', '(1 row)', '(2 rows)']:
                            tables_in_schema.append(table)

                # Check if we have both expected tables
                found_tables = [t for t in expected_tables if t in tables_in_schema]

                if len(found_tables) == len(expected_tables):
                    elapsed = int(time.time() - start_time)
                    print(f"  ‚úÖ All Trino tables found after {elapsed}s: {', '.join(found_tables)}")
                    return {
                        'success': True,
                        'tables': found_tables,
                        'elapsed': elapsed
                    }
                elif found_tables:
                    print(f"  ‚è≥ Partial tables found ({len(found_tables)}/{len(expected_tables)}): {', '.join(found_tables)}")

            except Exception as e:
                # Trino might not be ready yet or schema doesn't exist - keep waiting
                pass

            time.sleep(5)

        # Timeout
        elapsed = int(time.time() - start_time)
        return {
            'success': False,
            'timeout': True,
            'tables': found_tables,
            'expected': expected_tables,
            'elapsed': elapsed
        }

    def get_detailed_processing_status(self) -> Dict:
        """Get detailed breakdown of file processing status

        Returns:
            Dict with status counts, file details, and active tasks
        """
        if not self.provider_uuid:
            return {}

        try:
            # Get status breakdown with human-readable explanations
            status_result = self.db.execute_query("""
                SELECT
                    s.status,
                    COUNT(*) as count,
                    CASE s.status
                        WHEN 0 THEN 'PENDING - Waiting to download'
                        WHEN 1 THEN 'COMPLETE - Processing finished'
                        WHEN 2 THEN 'DOWNLOADING - Fetching from S3'
                        WHEN 3 THEN 'PROCESSING - Converting to parquet'
                        ELSE 'UNKNOWN'
                    END as status_name
                FROM reporting_common_costusagereportstatus s
                JOIN reporting_common_costusagereportmanifest m ON s.manifest_id = m.id
                WHERE m.provider_id = %s
                GROUP BY s.status
                ORDER BY s.status
            """, (self.provider_uuid,))

            # Get individual file details for in-progress files
            files_result = self.db.execute_query("""
                SELECT
                    s.report_name,
                    s.status,
                    s.last_started_datetime,
                    EXTRACT(EPOCH FROM (NOW() - s.last_started_datetime)) as elapsed_seconds
                FROM reporting_common_costusagereportstatus s
                JOIN reporting_common_costusagereportmanifest m ON s.manifest_id = m.id
                WHERE m.provider_id = %s
                AND s.status IN (2, 3)  -- DOWNLOADING or PROCESSING
                AND s.last_started_datetime IS NOT NULL
                ORDER BY s.last_started_datetime DESC
                LIMIT 3
            """, (self.provider_uuid,))

            return {
                'status_breakdown': status_result or [],
                'active_files': files_result or []
            }
        except Exception as e:
            return {'error': str(e)}

    def detect_pipeline_stage(self) -> str:
        """Detect current pipeline stage based on DB state

        Returns:
            Stage description with emoji indicator
        """
        if not self.provider_uuid:
            return "‚öôÔ∏è  Initializing"

        try:
            # Check file states to determine stage
            file_result = self.db.execute_query("""
                SELECT
                    COUNT(*) FILTER (WHERE status = 0) as pending,
                    COUNT(*) FILTER (WHERE status = 2) as downloading,
                    COUNT(*) FILTER (WHERE status = 3) as processing,
                    COUNT(*) FILTER (WHERE status = 1) as complete,
                    COUNT(*) as total
                FROM reporting_common_costusagereportstatus s
                JOIN reporting_common_costusagereportmanifest m ON s.manifest_id = m.id
                WHERE m.provider_id = %s
            """, (self.provider_uuid,))

            if file_result and file_result[0]:
                pending, downloading, processing, complete, total = file_result[0]

                if downloading > 0:
                    return f"‚¨áÔ∏è  Downloading files ({downloading}/{total} active)"
                elif processing > 0:
                    return f"‚öôÔ∏è  Processing & converting to parquet ({processing}/{total} active)"
                elif pending > 0:
                    return f"‚è≥ Files queued ({pending}/{total} waiting)"
                elif complete == total and total > 0:
                    return f"üìä Parquet conversion complete ({complete} files)"
                elif total > 0:
                    return f"üîÑ Processing ({complete}/{total} files complete)"

            return "üîÑ Processing"
        except Exception as e:
            return f"‚ùì Status check error: {str(e)[:40]}"

    def monitor_processing(self) -> Dict:
        """Monitor data processing with detailed progress reporting"""
        print(f"\n‚è≥ Monitoring processing (timeout: {self.timeout}s)...")
        if self.provider_uuid:
            print(f"   Provider: {self.provider_uuid}\n")

        start_count = self.check_processing_status()
        start_time = time.time()
        last_stage = None
        last_file_count = {}
        iteration = 0
        interval = 5  # Check every 5 seconds

        while True:
            elapsed = int(time.time() - start_time)

            if elapsed >= self.timeout:
                current_count = self.check_processing_status()
                print(f"\n  ‚è±Ô∏è  Timeout reached ({self.timeout}s)")
                if current_count > start_count:
                    print(f"  ‚úÖ Processing started ({current_count} manifests)")
                    return {
                        'success': True,
                        'timeout': True,
                        'manifest_count': current_count,
                        'elapsed': elapsed
                    }
                else:
                    print("  ‚ö†Ô∏è  No manifests processed")
                    return {
                        'success': False,
                        'timeout': True,
                        'manifest_count': current_count,
                        'elapsed': elapsed
                    }

            time.sleep(interval)
            iteration += 1

            # Get detailed status every iteration
            details = self.get_detailed_processing_status()
            current_stage = self.detect_pipeline_stage()
            current_count = self.check_processing_status()

            # Print stage change or periodic update (every 3rd iteration = 15s)
            stage_changed = current_stage != last_stage
            periodic_update = iteration % 3 == 0

            if stage_changed or periodic_update:
                print(f"\n  [{elapsed:3d}s] {current_stage}")

                # Print status breakdown
                if 'status_breakdown' in details and details['status_breakdown']:
                    for row in details['status_breakdown']:
                        status, count, status_name = row
                        prev_count = last_file_count.get(status, 0)

                        # Show delta if count changed
                        if count != prev_count and prev_count > 0:
                            delta = count - prev_count
                            delta_str = f" ({delta:+d})" if delta != 0 else ""
                            print(f"         ‚Ä¢ {status_name}: {count} file(s){delta_str}")
                        else:
                            print(f"         ‚Ä¢ {status_name}: {count} file(s)")

                        last_file_count[status] = count

                # Print active files with progress
                if details.get('active_files'):
                    print(f"         üìÇ Active files:")
                    for file_row in details['active_files']:
                        name, status, started, elapsed_s = file_row
                        status_icon = "‚¨áÔ∏è" if status == 2 else "‚öôÔ∏è"
                        elapsed_str = f"{int(elapsed_s)}s" if elapsed_s else "just started"
                        print(f"            {status_icon} {name[:40]}... ({elapsed_str})")

                # Show error hint if files are stuck
                if details.get('active_files'):
                    for file_row in details['active_files']:
                        _, status, _, elapsed_s = file_row
                        if elapsed_s and elapsed_s > 120:  # 2+ minutes
                            print(f"         ‚ö†Ô∏è  Warning: File processing for >2min (check worker logs/memory)")
                            break

                last_stage = current_stage

            # Check completion
            if current_count > start_count:
                print(f"\n  ‚úÖ Processing complete (elapsed: {elapsed}s)")
                print(f"  ‚ÑπÔ∏è  Total manifests: {current_count}")
                return {
                    'success': True,
                    'timeout': False,
                    'manifest_count': current_count,
                    'elapsed': elapsed
                }

    def run(self) -> Dict:
        """Run processing phase

        Returns:
            Results dict
        """
        print("\n" + "="*70)
        print("Phase 5-6: Data Processing")
        print("="*70 + "\n")

        # Fix any stuck reports from previous runs (makes script work in existing environments)
        if self.provider_uuid:
            print("üîß Checking for stuck reports from previous runs...")
            fix_result = self.fix_stuck_reports()

            if 'error' in fix_result:
                print(f"  ‚ö†Ô∏è  Error fixing stuck reports: {fix_result['error']}")
            elif fix_result['fixed'] > 0 or fix_result['cleared_task_ids'] > 0:
                print(f"  ‚úÖ Fixed {fix_result['fixed']} stuck report(s)")
                if fix_result['cleared_task_ids'] > 0:
                    print(f"  ‚úÖ Cleared {fix_result['cleared_task_ids']} stale task ID(s)")
            else:
                print(f"  ‚úì No stuck reports found")

        # Trigger processing
        print("\nüöÄ Triggering MASU data processing...")
        print(f"  Timeout: {self.timeout}s")
        if self.provider_uuid:
            print(f"  Provider UUID: {self.provider_uuid}")

        trigger_result = self.trigger_processing()

        if not trigger_result['success']:
            print(f"  ‚ùå Failed to trigger processing: {trigger_result.get('error')}")
            if 'output' in trigger_result:
                print(f"\n  Debug output:")
                for line in trigger_result['output'].split('\n')[:10]:
                    if line.strip():
                        print(f"    {line}")
            return {'passed': False, 'trigger': trigger_result}

        print(f"  ‚úÖ Task triggered: {trigger_result['task_id']}")
        print(f"     Task name: {trigger_result.get('task_name', 'unknown')}")

        if 'provider_type' in trigger_result:
            print(f"     Provider: {trigger_result['provider_name']} ({trigger_result['provider_type']})")
            print(f"     Org ID: {trigger_result['org_id']}")

        # Monitor processing
        monitor_result = self.monitor_processing()

        if monitor_result['success']:
            print(f"\n  ‚úÖ Processing complete")
            print(f"  ‚ÑπÔ∏è  Manifests: {monitor_result['manifest_count']}")
            print(f"  ‚ÑπÔ∏è  Time: {monitor_result['elapsed']}s")
        else:
            print(f"\n  ‚ö†Ô∏è  Processing timeout or incomplete")
            print(f"  ‚ÑπÔ∏è  Manifests: {monitor_result['manifest_count']}")
            print(f"  ‚ÑπÔ∏è  Elapsed: {monitor_result['elapsed']}s")

        # CRITICAL FIX: Mark manifests as complete to trigger summary
        # On-prem deployments don't auto-complete manifests after file processing
        # We do this even if monitoring timed out, as long as files are processed
        if self.provider_uuid:
            print(f"\nüìã Checking manifest completion status...")
            completion_result = self.mark_manifests_complete()

            if 'error' in completion_result:
                print(f"  ‚ö†Ô∏è  Error marking manifests complete: {completion_result['error']}")
            elif completion_result['marked_complete'] > 0:
                print(f"  ‚úÖ Marked {completion_result['marked_complete']} manifest(s) as complete")

                # If monitoring timed out but files are processed, consider it a success
                if not monitor_result['success'] and completion_result['marked_complete'] > 0:
                    print(f"  ‚ÑπÔ∏è  Manifests were completed manually (chord callback issue)")
                    monitor_result['success'] = True

                # Monitor summary table population with progress
                summary_result = self.monitor_summary_population(timeout=60)
                if not summary_result.get('has_data'):
                    if 'timeout' in summary_result:
                        print(f"  ‚ö†Ô∏è  Summary not populated after 60s (may need more time)")
                    elif 'error' in summary_result:
                        print(f"  ‚ö†Ô∏è  Summary check failed: {summary_result['error']}")

                # WORKAROUND: Create Hive schema and tables manually (Koku bug - only auto-creates for GCP)
                # Detect provider type from trigger result
                provider_type = trigger_result.get('provider_type', 'AWS')  # Default to AWS for backwards compatibility

                print(f"\nüîß Creating {provider_type} Hive schema and tables (workaround for Koku bug)...")
                hive_result = self.create_hive_tables_for_provider(provider_type)
                if hive_result.get('success'):
                    print(f"  ‚úÖ Created Hive schema '{self.org_id}' and tables for {provider_type}")
                    print(f"     Tables: {', '.join(hive_result.get('tables', []))}")
                    if not hive_result.get('tables'):
                        print(f"     Note: {provider_type} may not require daily tables")
                elif 'error' in hive_result:
                    print(f"  ‚ö†Ô∏è  Failed to create Hive tables for {provider_type}: {hive_result['error']}")

                # Wait for Trino tables to be created (parquet conversion is async)
                trino_result = self.wait_for_trino_tables(timeout=60)
                if not trino_result['success']:
                    if 'timeout' in trino_result:
                        print(f"  ‚ö†Ô∏è  Trino tables not ready after {trino_result['elapsed']}s (found: {trino_result.get('tables', [])})")
                    elif 'error' in trino_result:
                        print(f"  ‚ö†Ô∏è  Could not check Trino tables: {trino_result['error']}")
            else:
                print(f"  ‚ÑπÔ∏è  No manifests needed completion marking")

        return {
            'passed': monitor_result['success'],
            'trigger': trigger_result,
            'monitor': monitor_result
        }

